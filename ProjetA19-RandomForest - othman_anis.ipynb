{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# MTH3302 : Méthodes probabilistes et statistiques pour l'I.A.\n",
    "\n",
    "Jonathan Jalbert<br/>\n",
    "Professeur adjoint au Département de mathématiques et de génie industriel<br/>\n",
    "Polytechnique Montréal<br/>\n",
    "\n",
    "Le projet a été développé à l'aide de Alice Breton, étudiante à la maîtrise en génie informatique. Elle a suivi le cours lors de la session Hiver 2019.\n",
    "\n",
    "\n",
    "\n",
    "# Projet : Débordement d'égouts\n",
    "\n",
    "La description du projet est disponible à l'adresse suivante :\n",
    "https://www.kaggle.com/t/a238b752c33a41d9803c2cdde6bfc929\n",
    "\n",
    "Ce calepin Jupyter de base permet de charger et de nettoyer les données fournies. La dernière section détaille la génération du fichier des prédictions afin de le soumettre sur Kaggle dans le bon format.\n",
    "\n",
    "Dans un premier temps, vous devrez récupérer l'archive *data.zip* sur Moodle. Ce dossier contient les fichiers suivants :\n",
    "- surverses.csv\n",
    "- precipitation.csv\n",
    "- ouvrages-surverses.csv\n",
    "- test.csv\n",
    "\n",
    "Veuillez le décompresser dans le répertoire de ce calepin.\n",
    "\n",
    "Le fichier *surverse.csv* répertorie s'il y a surverse (1) ou non (0) au cours de la journée pour les 170 ouvrages de débordement de 2013 à 2018 pour les mois de mai à octobre (inclusivement). Des renseignements additionnels sur les données sont disponibles à l'adresse suivante :\n",
    "\n",
    "http://donnees.ville.montreal.qc.ca/dataset/debordement\n",
    "\n",
    "\n",
    "Le fichier *precipitation.csv* contient les précipitations horaires en dixième de *mm* enregistrées à 5 stations pluviométriques de 2013 à 2019 :\n",
    "- McTavish (7024745)\n",
    "- Ste-Anne-de-Bellevue (702FHL8)\n",
    "- Montreal/Pierre Elliott Trudeau Intl (702S006)\n",
    "- Montreal/St-Hubert (7027329)\n",
    "- L’Assomption (7014160)\n",
    "\n",
    "Plus d'informations sur les précipitations sont disponibles à l'adresse suivante :\n",
    "\n",
    "https://climat.meteo.gc.ca/climate_data/hourly_data_f.html?hlyRange=2008-01-08%7C2019-11-12&dlyRange=2002-12-23%7C2019-11-12&mlyRange=%7C&StationID=30165&Prov=QC&urlExtension=_f.html&searchType=stnName&optLimit=yearRange&StartYear=1840&EndYear=2019&selRowPerPage=25&Line=17&searchMethod=contains&Month=11&Day=12&txtStationName=montreal&timeframe=1&Year=2019\n",
    "\n",
    "Le fichier *ouvrages-surverses.csv* contient différentes caractéristiques des ouvrages de débordement. \n",
    "\n",
    "http://donnees.ville.montreal.qc.ca/dataset/ouvrage-surverse\n",
    "\n",
    "Le fichier *test.csv* contient les ouvrages et les jours pour lesquels vous devez prédire s'il y a eu surverse (true) ou non (false). Notez que l'on s'intéresse ici à 5 ouvrages de débordement localisés tout autour de l'Ile de Montréal :\n",
    "- 3260-01D dans Rivière-des-Prairies \n",
    "- 3350-07D dans Ahunstic \n",
    "- 4240-01D dans Pointe-aux-Trembles \n",
    "- 4350-01D dans le Vieux-Montréal \n",
    "- 4380-01D dans Verdun\n",
    "\n",
    "#### Remarque\n",
    "\n",
    "Dans le projet, on ne s'intéresse qu'aux surverses occasionnées par les précipitations. On ignore les surverses occasionnées par \n",
    "- fonte de neige (F)\n",
    "- travaux planifiés et entretien (TPL)\n",
    "- urgence (U)\n",
    "- autre (AUT)\n",
    "\n",
    "On suppose que lorsqu'il n'y a pas de raison pour la surverse, il s'agit d'une surverse causée par les précipitations. Puisque Nous nous intéresserons uniquement aux surverses occasionnées par les précipitations liquides, nous ne considérons que les mois de mai à octobre inclusivement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using CSV, DataFrames, Statistics, Dates, Gadfly, LinearAlgebra, Random\n",
    "using ScikitLearn, DecisionTree\n",
    "\n",
    "@sk_import decomposition: PCA\n",
    "@sk_import linear_model: LogisticRegression\n",
    "@sk_import preprocessing: StandardScaler\n",
    "@sk_import model_selection: (train_test_split, RandomizedSearchCV, GridSearchCV)\n",
    "@sk_import metrics: (classification_report, roc_auc_score, auc, make_scorer, f1_score)\n",
    "# @sk_import model_selection: train_test_split\n",
    "\n",
    "\n",
    "\n",
    "#, GLM,Distributions,\n",
    "include(\"functions.jl\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chargement des données et nettoyage préliminaire"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chargement et nettoyage des surverses et précipitations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = CSV.read(\"data/surverses.csv\",missingstring=\"-99999\")\n",
    "first(data,5)\n",
    "\n",
    "# Extraction des surverses pour les mois de mai à octobre inclusivement\n",
    "data = filter(row -> month(row.DATE) > 4, data) \n",
    "data = filter(row -> month(row.DATE) < 11, data) \n",
    "\n",
    "# Remplacement des valeurs *missing* dans la colonne :RAISON par \"Inconnue\"\n",
    "raison = coalesce.(data[:,:RAISON],\"Inconnue\")\n",
    "data[!,:RAISON] = raison\n",
    "\n",
    "# Exlusion des surverses coccasionnées par d'autres facteurs que les précipitations liquides\n",
    "data = filter(row -> row.RAISON ∈ [\"P\",\"Inconnue\",\"TS\"], data) \n",
    "select!(data, [:NO_OUVRAGE, :DATE, :SURVERSE])\n",
    "\n",
    "surverse_df = dropmissing(data, disallowmissing=true)\n",
    "\n",
    "### CHARGEMENT DES PRÉCIPITATIONS\n",
    "data = CSV.read(\"data/precipitations.csv\",missingstring=\"-99999\")\n",
    "rename!(data, Symbol(\"St-Hubert\")=>:StHubert)\n",
    "\n",
    "# Nettoyage des données sur les précipitations\n",
    "data = filter(row -> month(row.date) > 4, data) \n",
    "data = filter(row -> month(row.date) < 11, data);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# REMPLACER LES VALEURS MANQUANTES DANS LE DATAFRAME DE PRÉCIPITATIONS\n",
    "# On veut remplacer les données manquante grâce à une simple régression linéaire (Ridge).\n",
    "# Pour ce faire, chaque station météo agira comme variable explicative pour les 4 autres\n",
    "# On drop les rangées dont les 5valeurs sont missing\n",
    "\n",
    "data_full = deepcopy(data)\n",
    "dropmissing!(data, disallowmissing=true)\n",
    "\n",
    "models = Dict()\n",
    "\n",
    "for i = 1:length(data_full[:, 1])\n",
    "    if any(ismissing, data_full[i, 3:7]) #ismissing c est pas une variable mais une méthode de Julia, 3:7 pour les 5 stations\n",
    "        missing = []\n",
    "        notmissing = []\n",
    "        for j=3:7 #on passe à travers les 5 colonnes pour regarder celles qui sont missing ou non\n",
    "            if ismissing(data_full[i, j])\n",
    "                push!(missing, names(data_full)[j]) #push généralisé à tous les conteneurs\n",
    "                #Le point d exclamation pour dire à la fonction de modifier le conteneur qu on lui passe\n",
    "            else\n",
    "                push!(notmissing, names(data_full)[j])\n",
    "            end\n",
    "        end\n",
    "        \n",
    "        if length(notmissing) > 0 #si on a 5 missing sur la ligne on peut juste rien prédire donc on skip ça\n",
    "            for m in missing #m c est la station à prédire\n",
    "                if !haskey(models, (m, notmissing)) #ça véfirie si dans le dictionnaire models il y a la key (m, notmissing)\n",
    "                    models[(m, notmissing)] = CoeffRidge(data, m, notmissing) #on change la case dans le dico palr le coef que la fonction nous retourne\n",
    "                end\n",
    "                data_full[i, m] = round(convert(Vector{Float64}, data_full[i, notmissing])' * models[(m, notmissing)]) # on ajoute à la case manque la nouvelle valeur = x*Beta\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "end\n",
    "\n",
    "dropmissing!(data_full);\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enlever les données aberrantes somehow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_full[3902, 3] = 0\n",
    "data_full[3902, 7] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extraction variables explicatives (Maude L.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Maximum de précipitations en 1 heure dans une journée.\n",
    "# X_pcp_max = by(data_full, :date,  MAXMcTavish = :McTavish=>maximum, MAXBellevue = :Bellevue=>maximum, \n",
    "#    MAXAssomption = :Assomption=>maximum, MAXTrudeau = :Trudeau=>maximum, MAXStHubert = :StHubert=>maximum)\n",
    "\n",
    "# # Somme des précipitations dans une journée\n",
    "# X_pcp_sum = by(data_full, :date,  SUMMcTavish = :McTavish=>sum, SUMBellevue = :Bellevue=>sum, \n",
    "#    SUMAssomption = :Assomption=>sum, SUMTrudeau = :Trudeau=>sum, SUMStHubert = :StHubert=>sum)\n",
    "\n",
    "# # Précipitation à chaque heure pour chaque journée\n",
    "# X_pcp_hour = data_full\n",
    "\n",
    "# # Somme des précipitations des 2 heures précédentes\n",
    "\n",
    "# X_pcp_three_hours = copy(data_full)\n",
    "# buffer = copy(data_full)\n",
    "# for i=3:7\n",
    "#     for j=1:length(data_full[:,2])\n",
    "#         if (j > 2)\n",
    "#             X_pcp_three_hours[j, i] = (buffer[(j-2), i] + buffer[(j-1), i] + buffer[j,i])\n",
    "#         else\n",
    "#             X_pcp_three_hours[j, i] = buffer[j, i]\n",
    "#         end\n",
    "#     end\n",
    "# end\n",
    "\n",
    "# # Somme des précipitations des 2 jours précédents\n",
    "\n",
    "# X_pcp_two_days = DataFrame(date= X_pcp_sum[:,:date], SUM2DaysMcTavish=zeros(Int64, length(X_pcp_sum[:,1])), SUM2DaysBellevue=zeros(Int64, length(X_pcp_sum[:,1])), SUM2DaysAssomption=zeros(Int64, length(X_pcp_sum[:,1])), SUM2DaysTrudeau=zeros(Int64, length(X_pcp_sum[:,1])), SUM2DaysStHubert=zeros(Int64, length(X_pcp_sum[:,1])))\n",
    "# #buffer = copy(X_pcp_sum)\n",
    "# for i=2:6\n",
    "#     for j=1:length(X_pcp_two_days[:,1])\n",
    "#         if (j > 1)\n",
    "#             X_pcp_two_days[j, i] = X_pcp_sum[(j-1), i] + X_pcp_sum[j,i]\n",
    "#         else\n",
    "#             X_pcp_two_days[j, i] = X_pcp_sum[j, i]\n",
    "#         end\n",
    "#     end\n",
    "# end\n",
    "\n",
    "\n",
    "# # Mois de la prédiction pour chaque ouvrage\n",
    "# X_date = DataFrame(NO_OUVRAGE=surverse_df[:,:NO_OUVRAGE], MOIS=zeros(Int64,length(surverse_df[:,1])), SURVERSE=surverse_df[:,:SURVERSE])\n",
    "# for i=1:length(surverse_df[:,1])\n",
    "#     X_date[i,:MOIS] = month(surverse_df[i,:DATE])\n",
    "# end\n",
    "\n",
    "# X_mois = DataFrame(MOIS=[\"Mai\", \"Juin\", \"Juillet\", \"Août\", \"Septembre\", \"Octobre\"], RivierePrairie=zeros(Int64,6), Ahuntsic=zeros(Int64,6), PointeAuxTrembles=zeros(Int64,6), VieuxMontreal=zeros(Int64,6), Verdun=zeros(Int64,6))\n",
    "# for j=1:length(X_date[:,1])\n",
    "#     if(X_date[j,1] == \"3260-01D\")\n",
    "#         X_mois[(X_date[j,2]-4),:RivierePrairie] += (X_date[j,3])\n",
    "#     elseif (X_date[j,1] == \"3350-07D\")\n",
    "#         X_mois[(X_date[j,2]-4),:Ahuntsic] += (X_date[j,3])\n",
    "#     elseif (X_date[j,1] == \"4240-01D\")\n",
    "#         X_mois[(X_date[j,2]-4),:PointeAuxTrembles] += (X_date[j,3])\n",
    "#     elseif (X_date[j,1] == \"4350-01D\")\n",
    "#         X_mois[(X_date[j,2]-4),:VieuxMontreal] += (X_date[j,3])\n",
    "#     elseif (X_date[j,1] == \"4380-01D\")\n",
    "#         X_mois[(X_date[j,2]-4),:Verdun] += (X_date[j,3])\n",
    "#     end\n",
    "# end\n",
    "\n",
    "##Anis\n",
    "# Maximum de précipitations en 1 heure dans une journée.\n",
    "X_pcp_max = by(data_full, :date,  MAXMcTavish = :McTavish=>maximum, MAXBellevue = :Bellevue=>maximum, \n",
    "   MAXAssomption = :Assomption=>maximum, MAXTrudeau = :Trudeau=>maximum, MAXStHubert = :StHubert=>maximum)\n",
    "\n",
    "# Somme des précipitations dans une journée\n",
    "X_pcp_sum = by(data_full, :date,  SUMMcTavish = :McTavish=>sum, SUMBellevue = :Bellevue=>sum, \n",
    "   SUMAssomption = :Assomption=>sum, SUMTrudeau = :Trudeau=>sum, SUMStHubert = :StHubert=>sum)\n",
    "\n",
    "# Précipitation à chaque heure pour chaque journée\n",
    "X_pcp_hour = data_full\n",
    "\n",
    "# Somme des précipitations des 2 heures précédentes\n",
    "\n",
    "X_pcp_three_hours = copy(data_full)\n",
    "buffer = copy(data_full)\n",
    "for i=3:7\n",
    "    for j=1:length(data_full[:,2])\n",
    "        if (j > 2)\n",
    "            X_pcp_three_hours[j, i] = (buffer[(j-2), i] + buffer[(j-1), i] + buffer[j,i])\n",
    "        else\n",
    "            X_pcp_three_hours[j, i] = buffer[j, i]\n",
    "        end\n",
    "    end\n",
    "end\n",
    "\n",
    "# Somme des précipitations des 2 jours précédents\n",
    "\n",
    "X_pcp_two_days = DataFrame(date= X_pcp_sum[:,:date], SUM2DaysMcTavish=zeros(Int64, length(X_pcp_sum[:,1])), SUM2DaysBellevue=zeros(Int64, length(X_pcp_sum[:,1])), SUM2DaysAssomption=zeros(Int64, length(X_pcp_sum[:,1])), SUM2DaysTrudeau=zeros(Int64, length(X_pcp_sum[:,1])), SUM2DaysStHubert=zeros(Int64, length(X_pcp_sum[:,1])))\n",
    "#buffer = copy(X_pcp_sum)\n",
    "for i=2:6\n",
    "    for j=1:length(X_pcp_two_days[:,1])\n",
    "        if (j > 1)\n",
    "            X_pcp_two_days[j, i] = X_pcp_sum[(j-1), i] + X_pcp_sum[j,i]\n",
    "        else\n",
    "            X_pcp_two_days[j, i] = X_pcp_sum[j, i]\n",
    "        end\n",
    "    end\n",
    "end\n",
    "\n",
    "# Moyenne des précipitations avec les 3 dernières heures\n",
    "\n",
    "X_mean_pcp_three_hours = copy(data_full)\n",
    "buffer = copy(data_full)\n",
    "for i=3:7\n",
    "    for j=1:length(data_full[:,3])\n",
    "        if (j > 3)\n",
    "            X_mean_pcp_three_hours[j, i] = floor((buffer[(j-3), i] + buffer[(j-2), i] + buffer[(j-1), i] + buffer[j,i]) / 4)\n",
    "        else\n",
    "            X_mean_pcp_three_hours[j, i] = buffer[j, i]\n",
    "        end\n",
    "    end\n",
    "end\n",
    "\n",
    "println(X_mean_pcp_three_hours[1:5, :])\n",
    "\n",
    "# Mediane des précipitations en 1 heure dans une journée.\n",
    "X_pcp_median = by(data_full, :date,  MEDIANMcTavish = :McTavish=>median, MEDIANBellevue = :Bellevue=>median, \n",
    "   MEDIANAssomption = :Assomption=>median, MEDIANTrudeau = :Trudeau=>median, MEDIANStHubert = :StHubert=>median)\n",
    "\n",
    "\n",
    "# Mois de la prédiction pour chaque ouvrage\n",
    "X_date = DataFrame(NO_OUVRAGE=surverse_df[:,:NO_OUVRAGE], MOIS=zeros(Int64,length(surverse_df[:,1])), SURVERSE=surverse_df[:,:SURVERSE])\n",
    "for i=1:length(surverse_df[:,1])\n",
    "    X_date[i,:MOIS] = month(surverse_df[i,:DATE])\n",
    "end\n",
    "\n",
    "X_mois = DataFrame(MOIS=[\"Mai\", \"Juin\", \"Juillet\", \"Août\", \"Septembre\", \"Octobre\"], RivierePrairie=zeros(Int64,6), Ahuntsic=zeros(Int64,6), PointeAuxTrembles=zeros(Int64,6), VieuxMontreal=zeros(Int64,6), Verdun=zeros(Int64,6))\n",
    "for j=1:length(X_date[:,1])\n",
    "    if(X_date[j,1] == \"3260-01D\")\n",
    "        X_mois[(X_date[j,2]-4),:RivierePrairie] += (X_date[j,3])\n",
    "    elseif (X_date[j,1] == \"3350-07D\")\n",
    "        X_mois[(X_date[j,2]-4),:Ahuntsic] += (X_date[j,3])\n",
    "    elseif (X_date[j,1] == \"4240-01D\")\n",
    "        X_mois[(X_date[j,2]-4),:PointeAuxTrembles] += (X_date[j,3])\n",
    "    elseif (X_date[j,1] == \"4350-01D\")\n",
    "        X_mois[(X_date[j,2]-4),:VieuxMontreal] += (X_date[j,3])\n",
    "    elseif (X_date[j,1] == \"4380-01D\")\n",
    "        X_mois[(X_date[j,2]-4),:Verdun] += (X_date[j,3])\n",
    "    end\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création de 5 dataframes séparés pour les données de surverses des 5 ouvrages d'intérêt\n",
    "ouvrage = [\"3260-01D\", \"3350-07D\", \"4240-01D\", \"4350-01D\", \"4380-01D\"]\n",
    "\n",
    "filtered_surverse = filter(row -> row.NO_OUVRAGE == ouvrage[1], surverse_df)\n",
    "surverse_rivierePrairie = DataFrame(DATE=filtered_surverse[:, 2], SURVERSE=filtered_surverse[:, 3])\n",
    "\n",
    "filtered_surverse = filter(row -> row.NO_OUVRAGE == ouvrage[2], surverse_df)\n",
    "surverse_ahuntsic = DataFrame(DATE=filtered_surverse[:, 2], SURVERSE=filtered_surverse[:, 3])\n",
    "\n",
    "filtered_surverse = filter(row -> row.NO_OUVRAGE == ouvrage[3], surverse_df)\n",
    "surverse_pointeAuxTrembles = DataFrame(DATE=filtered_surverse[:, 2], SURVERSE=filtered_surverse[:, 3])\n",
    "\n",
    "filtered_surverse = filter(row -> row.NO_OUVRAGE == ouvrage[4], surverse_df)\n",
    "surverse_vieuxMontreal = DataFrame(DATE=filtered_surverse[:, 2], SURVERSE=filtered_surverse[:, 3])\n",
    "\n",
    "filtered_surverse = filter(row -> row.NO_OUVRAGE == ouvrage[5], surverse_df)\n",
    "surverse_verdun = DataFrame(DATE=filtered_surverse[:, 2], SURVERSE=filtered_surverse[:, 3]);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Création de variables explicatives représentant la densité de pluie tombée selon les heures\n",
    "Afin de prendre en considération une quantité de précipitations importantes tombées en une courte période de temps, nous avons calculé pour chaque heure de la journée, la somme des précipitations des deux heures précédentes avec les précipitations de l'heure courante. À partir de cela, nous avons créé la variable MAXPCP3HOURS qui présente la quantité maximale de pluie par bloc de 3 heures dans une journée, et la variable NUMBLOCK qui représente le nombre de bloc de 3 heures ayant une somme de précipitations plus élevé qu'un seuil déterminé en essayant différents seuil et en représentant graphiquement la discrimination entre les journées avec surverse et sans surverse, selon le seuil choisi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function aboveThreshold(x::Array{Int64,1}, threshold::Int64)\n",
    "    num = 0\n",
    "    for j=1:length(x)\n",
    "        if x[j] > threshold\n",
    "            num += 1\n",
    "        end\n",
    "    end\n",
    "    return num\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Dataframe avec le maximum de précipitations dans chaque bloc de 3 heures, et le nombre de bloc de 3 heures\n",
    "## ayant eu plus que 75 mm de précipitations.\n",
    "df_3h = DataFrame(date=X_pcp_sum[:, :date], MAXBLOCMcTavish=zeros(Int64, length(X_pcp_sum[:,1])), MAXBLOCBellevue=zeros(Int64, length(X_pcp_sum[:,1])), MAXBLOCAssomption=zeros(Int64, length(X_pcp_sum[:,1])), MAXBLOCTrudeau=zeros(Int64, length(X_pcp_sum[:,1])), MAXBLOCStHubert=zeros(Int64, length(X_pcp_sum[:,1])), BLOCMcTavish=zeros(Int64, length(X_pcp_sum[:,1])), BLOCBellevue=zeros(Int64, length(X_pcp_sum[:,1])), BLOCAssomption=zeros(Int64, length(X_pcp_sum[:,1])), BLOCTrudeau=zeros(Int64, length(X_pcp_sum[:,1])), BLOCStHubert=zeros(Int64, length(X_pcp_sum[:,1])))\n",
    "for i=1:length(X_pcp_sum[:,1])-1\n",
    "    for j=2:6\n",
    "        df_3h[i, j] = findmax(X_pcp_three_hours[((i-1)*24)+1:i*24, j+1])[1]\n",
    "    end\n",
    "    for j=7:11\n",
    "        df_3h[i, j] = aboveThreshold(X_pcp_three_hours[((i-1)*24)+1:i*24, j-4], 75)\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## X_max_3h est la variable avec le maximum de précipitations par 3 heures\n",
    "## X_bloc_3h est la variable avec le nombre de blocs ayant eu plus que 75 mm de précipitations\n",
    "X_max_3h = df_3h[!, 1:6]\n",
    "X_bloc_3h = df_3h[!, 7:11]\n",
    "insert!(X_bloc_3h, 1, df_3h[!, 1], :date);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Dataframe pour le plot d'en dessous\n",
    "df_3h_3260 = copy(df_3h)\n",
    "df_3h_3260.SURVERSE = zeros(Int64, length(df_3h[:,1]))\n",
    "missing = []\n",
    "for i=1:length(df_3h_3260[:,1])\n",
    "    surverse = filter(x -> x.DATE == df_3h[i, :date], surverse_vieuxMontreal).SURVERSE\n",
    "    if length(surverse) != 0\n",
    "        df_3h_3260[i, :SURVERSE] = surverse[1]\n",
    "    else\n",
    "        push!(missing, i)\n",
    "    end\n",
    "end\n",
    "deleterows!(df_3h_3260, missing);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Plot pour déterminer ish le seuil de précipitations par bloc de 3 heures\n",
    "plot(df_3h_3260,x=\"MAXBLOCMcTavish\", y=\"BLOCMcTavish\", color=:SURVERSE, Geom.beeswarm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyse exploratoire\n",
    "\n",
    "Cette section consitue une analyse exploratoire superficielle permettant de voir s'il existe un lien entre les précipitations et les surverses.\n",
    "\n",
    "Prenons arbitrairement l'ouvrage de débordement près du Bota-Bota (4350-01D). La station météorologique la plus proche est McTavish. Prenons deux variables explicatives simple :\n",
    "- la somme journalière des précipitations\n",
    "- le taux horaire maximum journalier de précipitations\n",
    "\n",
    "#### Calcul de la quantité journalière de précipitations pour chacune des stations météorologiques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modèle #1: Classification par Random Forests\n",
    "\n",
    "### Écrire une description "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CreateDataframe (generic function with 1 method)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # Construction d'un dataframe contenant toutes les variables explicatives\n",
    "\n",
    "# function CreateDataframe(ouvrage, surverse_df)\n",
    "#     df_train = filter(row -> row.NO_OUVRAGE == ouvrage, surverse_df)\n",
    "\n",
    "#     x₁ = Array{Int64}(undef, size(df_train,1)) # variable pour la somme journalière McTavish\n",
    "#     x₂ = Array{Int64}(undef, size(df_train,1)) # variable pour la somme journalière Bellevue\n",
    "#     x₃ = Array{Int64}(undef, size(df_train,1)) # variable pour la somme journalière Assomption\n",
    "#     x₄ = Array{Int64}(undef, size(df_train,1)) # variable pour la somme journalière PET\n",
    "#     x₅ = Array{Int64}(undef, size(df_train,1)) # variable pour la somme journalière St-Hubert\n",
    "#     x₆ = Array{Int64}(undef, size(df_train,1)) # variable pour la somme des 2 jours McTavish\n",
    "#     x₇ = Array{Int64}(undef, size(df_train,1)) # variable pour la somme des 2 jours Bellevue\n",
    "#     x₈ = Array{Int64}(undef, size(df_train,1)) # variable pour la somme des 2 jours Assomption\n",
    "#     x₉ = Array{Int64}(undef, size(df_train,1)) # variable pour la somme des 2 jours PET\n",
    "#     x₁₀ = Array{Int64}(undef, size(df_train,1)) # variable pour la somme des 2 jours St-Hubert\n",
    "#     x₁₁ = Array{Int64}(undef, size(df_train,1)) # variable pour le max journalier McTavish\n",
    "#     x₁₂ = Array{Int64}(undef, size(df_train,1)) # variable pour le max journalier Bellevue\n",
    "#     x₁₃ = Array{Int64}(undef, size(df_train,1)) # variable pour le max journalier Assomption\n",
    "#     x₁₄ = Array{Int64}(undef, size(df_train,1)) # variable pour le max journalier PET\n",
    "#     x₁₅ = Array{Int64}(undef, size(df_train,1)) # variable pour le max journalier St-Hubert\n",
    "#     x₁₆ = Array{Int64}(undef, size(df_train,1)) # variable pour le max par bloc de 3h McTavish\n",
    "#     x₁₇ = Array{Int64}(undef, size(df_train,1)) # variable pour le max par bloc de 3h Bellevue\n",
    "#     x₁₈ = Array{Int64}(undef, size(df_train,1)) # variable pour le max par bloc de 3h Assomption\n",
    "#     x₁₉ = Array{Int64}(undef, size(df_train,1)) # variable pour le max par bloc de 3h PET\n",
    "#     x₂₀ = Array{Int64}(undef, size(df_train,1)) # variable pour le max par bloc de 3h St-Hubert\n",
    "#     x₂₁ = Array{Int64}(undef, size(df_train,1)) # variable pour le nombre de blocs de 3h avec plus de 75 mm de pluie McTavish\n",
    "#     x₂₂ = Array{Int64}(undef, size(df_train,1)) # variable pour le nombre de blocs de 3h avec plus de 75 mm de pluie Bellevue\n",
    "#     x₂₃ = Array{Int64}(undef, size(df_train,1)) # variable pour le nombre de blocs de 3h avec plus de 75 mm de pluie Assomption\n",
    "#     x₂₄ = Array{Int64}(undef, size(df_train,1)) # variable pour le nombre de blocs de 3h avec plus de 75 mm de pluie PET\n",
    "#     x₂₅ = Array{Int64}(undef, size(df_train,1)) # variable pour le nombre de blocs de 3h avec plus de 75 mm de pluie St-Hubert\n",
    "\n",
    "\n",
    "\n",
    "#     for i=1:size(df_train,1)\n",
    "\n",
    "#         ind = findfirst(X_pcp_sum[:,:date] .== df_train[i,:DATE])\n",
    "        \n",
    "#         x₁[i] = X_pcp_sum[ind,:SUMMcTavish]\n",
    "\n",
    "#         x₂[i] = X_pcp_sum[ind,:SUMBellevue]\n",
    "\n",
    "#         x₃[i] = X_pcp_sum[ind,:SUMAssomption]\n",
    "\n",
    "#         x₄[i] = X_pcp_sum[ind,:SUMTrudeau]\n",
    "\n",
    "#         x₅[i] = X_pcp_sum[ind,:SUMStHubert]\n",
    "        \n",
    "#         ind = findfirst(X_pcp_two_days[:,:date] .== df_train[i,:DATE])\n",
    "\n",
    "#         x₆[i] = X_pcp_two_days[ind, :SUM2DaysMcTavish]\n",
    "\n",
    "#         x₇[i] = X_pcp_two_days[ind, :SUM2DaysBellevue]\n",
    "\n",
    "#         x₈[i] = X_pcp_two_days[ind, :SUM2DaysAssomption]\n",
    "\n",
    "#         x₉[i] = X_pcp_two_days[ind, :SUM2DaysTrudeau]\n",
    "\n",
    "#         x₁₀[i] = X_pcp_two_days[ind, :SUM2DaysStHubert]\n",
    "        \n",
    "#         ind = findfirst(X_pcp_max[:,:date] .== df_train[i,:DATE])\n",
    "\n",
    "#         x₁₁[i] = X_pcp_max[ind, :MAXMcTavish]\n",
    "\n",
    "#         x₁₂[i] = X_pcp_max[ind, :MAXBellevue]\n",
    "\n",
    "#         x₁₃[i] = X_pcp_max[ind, :MAXAssomption]\n",
    "\n",
    "#         x₁₄[i] = X_pcp_max[ind, :MAXTrudeau]\n",
    "\n",
    "#         x₁₅[i] = X_pcp_max[ind, :MAXStHubert]\n",
    "        \n",
    "#         ind = findfirst(X_max_3h[:,:date] .== df_train[i,:DATE])\n",
    "        \n",
    "#         x₁₆[i] = X_max_3h[ind, :MAXBLOCMcTavish]\n",
    "        \n",
    "#         x₁₇[i] = X_max_3h[ind, :MAXBLOCBellevue]\n",
    "        \n",
    "#         x₁₈[i] = X_max_3h[ind, :MAXBLOCAssomption]\n",
    "        \n",
    "#         x₁₉[i] = X_max_3h[ind, :MAXBLOCTrudeau]\n",
    "        \n",
    "#         x₂₀[i] = X_max_3h[ind, :MAXBLOCStHubert]\n",
    "        \n",
    "#         ind = findfirst(X_bloc_3h[:,:date] .== df_train[i,:DATE])\n",
    "        \n",
    "#         x₂₁[i] = X_bloc_3h[ind, :BLOCMcTavish]\n",
    "        \n",
    "#         x₂₂[i] = X_bloc_3h[ind, :BLOCBellevue]\n",
    "        \n",
    "#         x₂₃[i] = X_bloc_3h[ind, :BLOCAssomption]\n",
    "        \n",
    "#         x₂₄[i] = X_bloc_3h[ind, :BLOCTrudeau]\n",
    "        \n",
    "#         x₂₅[i] = X_bloc_3h[ind, :BLOCStHubert]\n",
    "\n",
    "#     end\n",
    "\n",
    "#     df_train[!,:SUMMcTavish] = x₁\n",
    "#     df_train[!,:SUMBellevue] = x₂\n",
    "#     df_train[!,:SUMAssomption] = x₃\n",
    "#     df_train[!,:SUMStHubert] = x₄\n",
    "#     df_train[!,:SUMPET] = x₅\n",
    "#     df_train[!,:SUM2McTavish] = x₆\n",
    "#     df_train[!,:SUM2Bellevue] = x₇\n",
    "#     df_train[!,:SUM2Assomption] = x₈\n",
    "#     df_train[!,:SUM2StHubert] = x₉\n",
    "#     df_train[!,:SUM2PET] = x₁₀\n",
    "#     df_train[!,:MAXMcTavish] = x₁₁\n",
    "#     df_train[!,:MAXBellevue] = x₁₂\n",
    "#     df_train[!,:MAXAssomption] = x₁₃\n",
    "#     df_train[!,:MAXStHubert] = x₁₄\n",
    "#     df_train[!,:MAXPET] = x₁₅\n",
    "#     df_train[!, :MAXBLOCMcTavish] = x₁₆\n",
    "#     df_train[!, :MAXBLOCBellevue] = x₁₇\n",
    "#     df_train[!, :MAXBLOCAssomption] = x₁₈\n",
    "#     df_train[!, :MAXBLOCTrudeau] = x₁₉\n",
    "#     df_train[!, :MAXBLOCStHubert] = x₂₀\n",
    "#     df_train[!, :BLOCMcTavish] = x₂₁\n",
    "#     df_train[!, :BLOCBellevue] = x₂₂\n",
    "#     df_train[!, :BLOCAssomption] = x₂₃\n",
    "#     df_train[!, :BLOCTrudeau] = x₂₄\n",
    "#     df_train[!, :BLOCStHubert] = x₂₅\n",
    "    \n",
    "#     return df_train\n",
    "\n",
    "# end\n",
    "\n",
    "\n",
    "# Construction d'un dataframe contenant toutes les variables explicatives\n",
    "\n",
    "function CreateDataframe(ouvrage, surverse_df)\n",
    "    df_train = filter(row -> row.NO_OUVRAGE == ouvrage, surverse_df)\n",
    "\n",
    "    x₁ = Array{Int64}(undef, size(df_train,1)) # variable pour la somme journalière McTavish\n",
    "    x₂ = Array{Int64}(undef, size(df_train,1)) # variable pour la somme journalière Bellevue\n",
    "    x₃ = Array{Int64}(undef, size(df_train,1)) # variable pour la somme journalière Assomption\n",
    "    x₄ = Array{Int64}(undef, size(df_train,1)) # variable pour la somme journalière PET\n",
    "    x₅ = Array{Int64}(undef, size(df_train,1)) # variable pour la somme journalière St-Hubert\n",
    "    x₆ = Array{Int64}(undef, size(df_train,1)) # variable pour la somme des 2 jours McTavish\n",
    "    x₇ = Array{Int64}(undef, size(df_train,1)) # variable pour la somme des 2 jours Bellevue\n",
    "    x₈ = Array{Int64}(undef, size(df_train,1)) # variable pour la somme des 2 jours Assomption\n",
    "    x₉ = Array{Int64}(undef, size(df_train,1)) # variable pour la somme des 2 jours PET\n",
    "    x₁₀ = Array{Int64}(undef, size(df_train,1)) # variable pour la somme des 2 jours St-Hubert\n",
    "    x₁₁ = Array{Int64}(undef, size(df_train,1)) # variable pour le max journalier McTavish\n",
    "    x₁₂ = Array{Int64}(undef, size(df_train,1)) # variable pour le max journalier Bellevue\n",
    "    x₁₃ = Array{Int64}(undef, size(df_train,1)) # variable pour le max journalier Assomption\n",
    "    x₁₄ = Array{Int64}(undef, size(df_train,1)) # variable pour le max journalier PET\n",
    "    x₁₅ = Array{Int64}(undef, size(df_train,1)) # variable pour le max journalier St-Hubert\n",
    "    x₁₆ = Array{Int64}(undef, size(df_train,1)) # variable pour le max par bloc de 3h McTavish\n",
    "    x₁₇ = Array{Int64}(undef, size(df_train,1)) # variable pour le max par bloc de 3h Bellevue\n",
    "    x₁₈ = Array{Int64}(undef, size(df_train,1)) # variable pour le max par bloc de 3h Assomption\n",
    "    x₁₉ = Array{Int64}(undef, size(df_train,1)) # variable pour le max par bloc de 3h PET\n",
    "    x₂₀ = Array{Int64}(undef, size(df_train,1)) # variable pour le max par bloc de 3h St-Hubert\n",
    "    x₂₁ = Array{Int64}(undef, size(df_train,1)) # variable pour le nombre de blocs de 3h avec plus de 75 mm de pluie McTavish\n",
    "    x₂₂ = Array{Int64}(undef, size(df_train,1)) # variable pour le nombre de blocs de 3h avec plus de 75 mm de pluie Bellevue\n",
    "    x₂₃ = Array{Int64}(undef, size(df_train,1)) # variable pour le nombre de blocs de 3h avec plus de 75 mm de pluie Assomption\n",
    "    x₂₄ = Array{Int64}(undef, size(df_train,1)) # variable pour le nombre de blocs de 3h avec plus de 75 mm de pluie PET\n",
    "    x₂₅ = Array{Int64}(undef, size(df_train,1)) # variable pour le nombre de blocs de 3h avec plus de 75 mm de pluie St-Hubert\n",
    "    x₂₆ = Array{Int64}(undef, size(df_train,1))    \n",
    "    x₂₇ = Array{Int64}(undef, size(df_train,1))\n",
    "    x₂₈ = Array{Int64}(undef, size(df_train,1))\n",
    "    x₂₉ = Array{Int64}(undef, size(df_train,1))\n",
    "    x₃₀ = Array{Int64}(undef, size(df_train,1))\n",
    "    x₃₁ = Array{Int64}(undef, size(df_train,1))\n",
    "    x₃₂ = Array{Int64}(undef, size(df_train,1))\n",
    "    x₃₃ = Array{Int64}(undef, size(df_train,1))\n",
    "    x₃₄ = Array{Int64}(undef, size(df_train,1))\n",
    "    x₃₅ = Array{Int64}(undef, size(df_train,1))\n",
    "\n",
    "\n",
    "\n",
    "    for i=1:size(df_train,1)\n",
    "\n",
    "        ind = findfirst(X_pcp_sum[:,:date] .== df_train[i,:DATE])\n",
    "        \n",
    "        x₁[i] = X_pcp_sum[ind,:SUMMcTavish]\n",
    "\n",
    "        x₂[i] = X_pcp_sum[ind,:SUMBellevue]\n",
    "\n",
    "        x₃[i] = X_pcp_sum[ind,:SUMAssomption]\n",
    "\n",
    "        x₄[i] = X_pcp_sum[ind,:SUMTrudeau]\n",
    "\n",
    "        x₅[i] = X_pcp_sum[ind,:SUMStHubert]\n",
    "        \n",
    "        ind = findfirst(X_pcp_two_days[:,:date] .== df_train[i,:DATE])\n",
    "\n",
    "        x₆[i] = X_pcp_two_days[ind, :SUM2DaysMcTavish]\n",
    "\n",
    "        x₇[i] = X_pcp_two_days[ind, :SUM2DaysBellevue]\n",
    "\n",
    "        x₈[i] = X_pcp_two_days[ind, :SUM2DaysAssomption]\n",
    "\n",
    "        x₉[i] = X_pcp_two_days[ind, :SUM2DaysTrudeau]\n",
    "\n",
    "        x₁₀[i] = X_pcp_two_days[ind, :SUM2DaysStHubert]\n",
    "        \n",
    "        ind = findfirst(X_pcp_max[:,:date] .== df_train[i,:DATE])\n",
    "\n",
    "        x₁₁[i] = X_pcp_max[ind, :MAXMcTavish]\n",
    "\n",
    "        x₁₂[i] = X_pcp_max[ind, :MAXBellevue]\n",
    "\n",
    "        x₁₃[i] = X_pcp_max[ind, :MAXAssomption]\n",
    "\n",
    "        x₁₄[i] = X_pcp_max[ind, :MAXTrudeau]\n",
    "\n",
    "        x₁₅[i] = X_pcp_max[ind, :MAXStHubert]\n",
    "        \n",
    "        ind = findfirst(X_max_3h[:,:date] .== df_train[i,:DATE])\n",
    "        \n",
    "        x₁₆[i] = X_max_3h[ind, :MAXBLOCMcTavish]\n",
    "        \n",
    "        x₁₇[i] = X_max_3h[ind, :MAXBLOCBellevue]\n",
    "        \n",
    "        x₁₈[i] = X_max_3h[ind, :MAXBLOCAssomption]\n",
    "        \n",
    "        x₁₉[i] = X_max_3h[ind, :MAXBLOCTrudeau]\n",
    "        \n",
    "        x₂₀[i] = X_max_3h[ind, :MAXBLOCStHubert]\n",
    "        \n",
    "        ind = findfirst(X_bloc_3h[:,:date] .== df_train[i,:DATE])\n",
    "        \n",
    "        x₂₁[i] = X_bloc_3h[ind, :BLOCMcTavish]\n",
    "        \n",
    "        x₂₂[i] = X_bloc_3h[ind, :BLOCBellevue]\n",
    "        \n",
    "        x₂₃[i] = X_bloc_3h[ind, :BLOCAssomption]\n",
    "        \n",
    "        x₂₄[i] = X_bloc_3h[ind, :BLOCTrudeau]\n",
    "        \n",
    "        x₂₅[i] = X_bloc_3h[ind, :BLOCStHubert]\n",
    "        \n",
    "        ind = findfirst(X_mean_pcp_three_hours[:,:date] .== df_train[i,:DATE])\n",
    "        \n",
    "        x₂₆[i] = X_mean_pcp_three_hours[ind, :McTavish]\n",
    "        \n",
    "        x₂₇[i] = X_mean_pcp_three_hours[ind, :Bellevue]\n",
    "        \n",
    "        x₂₈[i] = X_mean_pcp_three_hours[ind, :Assomption]\n",
    "        \n",
    "        x₂₉[i] = X_mean_pcp_three_hours[ind, :Trudeau]\n",
    "        \n",
    "        x₃₀[i] = X_mean_pcp_three_hours[ind, :StHubert]\n",
    "        \n",
    "        ind = findfirst(X_pcp_median[:,:date] .== df_train[i,:DATE])\n",
    "        \n",
    "        x₃₁[i] = floor(X_pcp_median[ind, :MEDIANMcTavish])\n",
    "        \n",
    "        x₃₂[i] = floor(X_pcp_median[ind, :MEDIANBellevue])\n",
    "        \n",
    "        x₃₃[i] = floor(X_pcp_median[ind, :MEDIANAssomption])\n",
    "            \n",
    "        x₃₄[i] = floor(X_pcp_median[ind, :MEDIANTrudeau])\n",
    "        \n",
    "        x₃₅[i] = floor(X_pcp_median[ind, :MEDIANStHubert])\n",
    "        \n",
    "\n",
    "    end\n",
    "\n",
    "    df_train[!,:SUMMcTavish] = x₁\n",
    "    df_train[!,:SUMBellevue] = x₂\n",
    "    df_train[!,:SUMAssomption] = x₃\n",
    "    df_train[!,:SUMStHubert] = x₄\n",
    "    df_train[!,:SUMPET] = x₅\n",
    "    df_train[!,:SUM2McTavish] = x₆\n",
    "    df_train[!,:SUM2Bellevue] = x₇\n",
    "    df_train[!,:SUM2Assomption] = x₈\n",
    "    df_train[!,:SUM2StHubert] = x₉\n",
    "    df_train[!,:SUM2PET] = x₁₀\n",
    "    df_train[!,:MAXMcTavish] = x₁₁\n",
    "    df_train[!,:MAXBellevue] = x₁₂\n",
    "    df_train[!,:MAXAssomption] = x₁₃\n",
    "    df_train[!,:MAXStHubert] = x₁₄\n",
    "    df_train[!,:MAXPET] = x₁₅\n",
    "    df_train[!, :MAXBLOCMcTavish] = x₁₆\n",
    "    df_train[!, :MAXBLOCBellevue] = x₁₇\n",
    "    df_train[!, :MAXBLOCAssomption] = x₁₈\n",
    "    df_train[!, :MAXBLOCTrudeau] = x₁₉\n",
    "    df_train[!, :MAXBLOCStHubert] = x₂₀\n",
    "    df_train[!, :BLOCMcTavish] = x₂₁\n",
    "    df_train[!, :BLOCBellevue] = x₂₂\n",
    "    df_train[!, :BLOCAssomption] = x₂₃\n",
    "    df_train[!, :BLOCTrudeau] = x₂₄\n",
    "    df_train[!, :BLOCStHubert] = x₂₅\n",
    "    df_train[!, :MEANMcTavish] = x₂₆\n",
    "    df_train[!, :MEANBellevue] = x₂₇\n",
    "    df_train[!, :MEANAssomption] = x₂₈\n",
    "    df_train[!, :MEANTrudeau] = x₂₉\n",
    "    df_train[!, :MEANStHubert] = x₃₀\n",
    "    df_train[!, :MEDIANMcTavish] = x₃₁\n",
    "    df_train[!, :MEDIANBellevue] = x₃₂\n",
    "    df_train[!, :MEDIANAssomption] = x₃₃\n",
    "    df_train[!, :MEDIANTrudeau] = x₃₄\n",
    "    df_train[!, :MEDIANStHubert] = x₃₅\n",
    "    \n",
    "    \n",
    "    return df_train\n",
    "\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "getForest"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "getForest permet d'obtenir pour chaque ouvrage un modèle de prédiction basé sur la classification par forêts aléatoires. \n",
    "Le paramètre rf_fit_params correspondent aux valeurs possibles des hyperparamètres de RandomForestClassifier\n",
    "\n",
    "@params\n",
    "num_ouvrage:      le numéro de l'ouvrage à évaluer\n",
    "rf_fit_param:     dictionnaire des valeurs possibles pour les hyperparamètres du modèle RF\n",
    "n_folds:          le nombre de plis à effectuer sur l'ensemble d'entrainement lors de la validation croisée\n",
    "grid:             false par défaut, si à true, la fonction effectue une GridSearch (exhaustive) sur l'ensemble des\n",
    "                  hyperparamètres fournis, sinon elle effectue une RandomizedSearch\n",
    "n_iter:           10 par défaut, le nombre de modèles à essayer lors d'une RandomizedSearch\n",
    "\n",
    "\"\"\"\n",
    "function getForest(num_ouvrage::String; fit_params::Dict, n_folds = 5, grid=false, n_iter = 1000, verbose=1)\n",
    "    train = CreateDataframe(num_ouvrage, surverse_df)\n",
    "    all_X = convert(Matrix{Float64}, train[:, 4:28])\n",
    "    all_y = train[!, :SURVERSE]\n",
    "    \n",
    "    # On doit utiliser la fonction train_test_split de sklearn car notre jeu de données est très désiquilibré.\n",
    "    # le parametre \"stratify\" de la fonction permet de parer à ca\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(all_X, all_y, test_size=.2, shuffle=true, random_state = 34, stratify=all_y)\n",
    "\n",
    "    # Préparation de la classification par RandomForests.\n",
    "    # Les HyperParamètres seront évalués dans la GridSearch/RandomizedSearch\n",
    "    # StandardScaler() est utilisé pour normaliser les données\n",
    "    pipe = Pipelines.Pipeline([(\"ss\", StandardScaler()),\n",
    "                               (\"rf\", RandomForestClassifier(rng=42))])\n",
    "    \n",
    "    n_models = 1\n",
    "    \n",
    "    for (k,v) in fit_params\n",
    "        n_models *= length(v)\n",
    "    end\n",
    "\n",
    "    println(\"Nombre de modèles à parcourir: $(n_models)\\n\")\n",
    "\n",
    "    if grid\n",
    "        M = GridSearch.GridSearchCV(estimator=pipe,\n",
    "                                    param_grid=fit_params,\n",
    "                                    cv=n_folds,\n",
    "                                    verbose=verbose)\n",
    "    else\n",
    "        M = GridSearch.RandomizedSearchCV(estimator=pipe,\n",
    "                                          param_distributions=fit_params,\n",
    "                                          cv=n_folds,\n",
    "                                          n_iter=n_iter,\n",
    "                                          verbose=verbose)\n",
    "    end\n",
    "\n",
    "    bon_fit = fit!(M, X_train, y_train)\n",
    "    println(\"training score: \", M.best_score_ , '\\n', \"best parameters: \", M.best_params_,'\\n')\n",
    "    y_true, y_pred = y_test, predict(M, X_test) \n",
    "    println(\"F1: \", f1_score(y_true, y_pred))\n",
    "    println(\"AUC: \",  roc_auc_score(y_true, y_pred))\n",
    "    println(\"test score: \", classification_report(y_true, y_pred))\n",
    "\n",
    "    return bon_fit\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest et RandomizedSearch/GridSearch\n",
    "## Démarche\n",
    "#### 1. RandomizedSearch\n",
    "Nous utilisons la technique de classification par RandomForests. Afin d'optimiser notre modèle, nous devons parcourir l'espace des hyperparamètres du modèle pour n'en garder que les meilleures valeurs: \n",
    "1. n_subfeatures: le nombre de variables explicatives à sélectionner aléatoirement pour chacun des arbres composant la forêt\n",
    "2. n_trees: le nombre d'arbres composant la forêt\n",
    "3. partial_sampling: la fraction du dataframe d'entrainement sur laquelle entrainer chacun des arbres\n",
    "4. max_depth: la profondeur maximale de chacun des arbres\n",
    "5. min_samples_leaf: le nombre minimal de données d'entrainement (\"sample\") par feuille\n",
    "6. min_sample_split: le nombre minimal de données d'entrainement requis pour qu'un noeud de l'arbre se sépare en deux autres noeuds\n",
    "\n",
    "Nous avons tout d'abord exécuté une RandomizedSearch de manière indépendante sur chacun des hyperparamètres en donnant à chaque fois une très grande portée à chaque paramètre. En modifiant ainsi chaque hyperparamètre un à un, nous avons pu réduire progressivement la portée possible de chaque paramètre en observant le score de chaque forêt en fonction du paramètre modifié. \n",
    "\n",
    "L'objectif de cette démarche a été de progressivement réduire l'espace des modèles pour pouvoir effectuer une GridSearch à la place d'une RandomizedSearch, et ce, sur l'entièreté des hyperparamètres\n",
    "\n",
    "#### 2. GridSearch\n",
    "\n",
    "La GridSearch, contrairement à la RandomizedSearch, permet de passer à travers chacun des modèles proposés, un à un. Comme pour la RandomizedSearch, elle utilise la validation croisée qui aide à réduire le phénomène de surapprentissage, en testant chacun des modèles plusieurs fois (k fois) sur un ensemble de test et d'entrainement différent à chaque fois."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de modèles à parcourir: 912384\n",
      "\n",
      "Fitting 5 folds for each of 250 candidates, totalling 1250 fits\n",
      "training score: 0.9555302166476625\n",
      "best parameters: Dict{Symbol,Any}(:rf__min_samples_leaf => 9,:rf__n_trees => 220,:rf__min_samples_split => 6,:rf__min_purity_increase => 0.1,:rf__n_subfeatures => 3,:rf__max_depth => 27,:rf__partial_sampling => 0.6)\n",
      "\n",
      "F1: 0.7333333333333334\n",
      "AUC: 0.7894736842105263\n",
      "test score:               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      1.00      0.98       201\n",
      "           1       1.00      0.58      0.73        19\n",
      "\n",
      "    accuracy                           0.96       220\n",
      "   macro avg       0.98      0.79      0.86       220\n",
      "weighted avg       0.97      0.96      0.96       220\n",
      "\n",
      " 97.355015 seconds (53.70 M allocations: 52.806 GiB, 9.27% gc time)\n",
      "Accuracy score: 0.9555302166476625\n",
      "\n",
      "3260-01D done\n",
      " ******************************* \n",
      " ******************************* \n",
      "Nombre de modèles à parcourir: 912384\n",
      "\n",
      "Fitting 5 folds for each of 250 candidates, totalling 1250 fits\n",
      "training score: 0.9313893653516295\n",
      "best parameters: Dict{Symbol,Any}(:rf__min_samples_leaf => 9,:rf__n_trees => 320,:rf__min_samples_split => 2,:rf__min_purity_increase => 0.1,:rf__n_subfeatures => 5,:rf__max_depth => 35,:rf__partial_sampling => 0.8)\n",
      "\n",
      "F1: 0.870967741935484\n",
      "AUC: 0.9284482758620689\n",
      "test score:               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.96      0.97       116\n",
      "           1       0.84      0.90      0.87        30\n",
      "\n",
      "    accuracy                           0.95       146\n",
      "   macro avg       0.91      0.93      0.92       146\n",
      "weighted avg       0.95      0.95      0.95       146\n",
      "\n",
      " 69.586205 seconds (32.82 M allocations: 35.269 GiB, 8.41% gc time)\n",
      "Accuracy score: 0.9313893653516295\n",
      "\n",
      "3350-07D done\n",
      " ******************************* \n",
      " ******************************* \n",
      "Nombre de modèles à parcourir: 912384\n",
      "\n",
      "Fitting 5 folds for each of 250 candidates, totalling 1250 fits\n"
     ]
    }
   ],
   "source": [
    "ouvrage = [\"3260-01D\", \"3350-07D\", \"4240-01D\", \"4350-01D\", \"4380-01D\"]\n",
    "best_models_rf = Dict()\n",
    "\n",
    "#Préparation d'un dictionnaire de paramètres pour l'optimisation des hyperparamètres du modèle\n",
    "#On déclare les valeurs jugées possibles pour le modèle\n",
    "\n",
    "rf_fit_params = Dict()\n",
    "rf_fit_params[\"rf__n_subfeatures\"] = 3:1:10 \n",
    "rf_fit_params[\"rf__n_trees\"] = 80:20:500\n",
    "rf_fit_params[\"rf__partial_sampling\"] = [0.6, 0.7, 0.8]\n",
    "rf_fit_params[\"rf__max_depth\"] = 3:2:50\n",
    "rf_fit_params[\"rf__min_samples_leaf\"] = 5:2:10\n",
    "rf_fit_params[\"rf__min_samples_split\"] = 2:2:16\n",
    "rf_fit_params[\"rf__min_purity_increase\"] = [0.0, 0.1, 0.2]\n",
    "\n",
    "for i=1:length(ouvrage)\n",
    "    @time best_models_rf[ouvrage[i]] = getForest(ouvrage[i],\n",
    "                                              fit_params = rf_fit_params,\n",
    "                                              n_iter=250)\n",
    "    println(\"Accuracy score: \", best_models_rf[ouvrage[i]].best_score_, \"\\n\")\n",
    "#     println(\"Model score: \", best_models_rf[ouvrage[i]].best_estimator_, \"\\n\")\n",
    "\n",
    "    println(ouvrage[i], \" done\")\n",
    "    println(\" ******************************* \")\n",
    "    println(\" ******************************* \")\n",
    "\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "getLogitPCA permet d'obtenir pour chaque ouvrage un modèle de prédiction basé sur la régression logistique et l'analyse\n",
    "en composantes principales. La fonction effectue une grid search sur chacune des configurations possibles proposées\n",
    "par le dictionnaire des hyoerparamètres fit_params\n",
    "\n",
    "@params\n",
    "num_ouvrage:      le numéro de l'ouvrage à évaluer\n",
    "fit_param:        dictionnaire des valeurs possibles pour les hyperparamètres du modèle de régression logistique\n",
    "n_folds:          le nombre de plis à effectuer sur l'ensemble d'entrainement lors de la validation croisée\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "function getLogitPCA(num_ouvrage::String; fit_params::Dict, n_folds=5, verbose=1)\n",
    "    train = CreateDataframe(num_ouvrage, surverse_df)\n",
    "    all_X = convert(Matrix{Float64}, train[:, 4:28])\n",
    "    all_y = train[!, :SURVERSE]\n",
    "    \n",
    "    # On doit utiliser la fonction train_test_split de sklearn car notre jeu de données est très désiquilibré.\n",
    "    # le parametre \"stratify\" de la fonction permet de parer à ca\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(all_X, all_y, test_size=.2, shuffle=true, random_state = 34, stratify=all_y)\n",
    "\n",
    "    n_models = 1\n",
    "    \n",
    "    for (k,v) in fit_params\n",
    "        n_models *= length(v)\n",
    "    end\n",
    "\n",
    "    println(\"Nombre de modèles à parcourir: $(n_models)\\n\")\n",
    "    \n",
    "    # Préparation du Pipeline à tester.\n",
    "    # Le Pipeline permet de lier la standardisation des données à l'obtention des composantes principales et à la création \n",
    "    # d'une régression logistique les utilisant.\n",
    "    pipe = Pipelines.Pipeline([(\"ss\", StandardScaler()),\n",
    "                               (\"pca\", PCA()),\n",
    "                               (\"lr\", LogisticRegression(max_iter=1000, random_state=42))])\n",
    "    \n",
    "    M = GridSearch.GridSearchCV(estimator=pipe,\n",
    "                                param_grid=fit_params,\n",
    "                                cv=n_folds, \n",
    "                                verbose=verbose)\n",
    "\n",
    "    return fit!(M, X_train, y_train)\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_fit_params = Dict()\n",
    "lr_fit_params[\"lr__C\"] = [0.0001, 0.001, 0.01, 0.1, 1.0, 10.0]#exp10.(-4: 1: 1)\n",
    "lr_fit_params[\"pca__n_components\"] = 2:1:15\n",
    "\n",
    "ouvrage = [\"3260-01D\", \"3350-07D\", \"4240-01D\", \"4350-01D\", \"4380-01D\"]\n",
    "best_models_lr = Dict()\n",
    "\n",
    "for i=1:length(ouvrage)\n",
    "    @time best_models_lr[ouvrage[i]] = getLogitPCA(ouvrage[i], fit_params=lr_fit_params)\n",
    "    println(\"Accuracy score: \", best_models_lr[ouvrage[i]].best_score_)\n",
    "    println(ouvrage[i], \" done\\n\")\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PAS SÛR À 100% QU'IL FAILLE VRAIMENT FAIRE ÇA MAIS ÇA ME SEMBLAIT PAS MAL\n",
    "# On choisit maintenant, pour chacun des ouvrages, le modèle ayant la meilleur accuracy entre la RandomForest et la \n",
    "# régression logistique\n",
    "\n",
    "# best_models = Dict()\n",
    "\n",
    "# for (ouvrage, modele) in best_models_rf\n",
    "#     if modele.best_score_ > best_models_lr[ouvrage].best_score_\n",
    "#         best_models[ouvrage] = modele\n",
    "#     else\n",
    "#         best_models[ouvrage] = best_models_lr[ouvrage]\n",
    "#     end\n",
    "# end\n",
    "best_models = best_models_rf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Création du fichier de prédictions pour soumettre sur Kaggle\n",
    "\n",
    "Dans ce cas-ci, nous prédirons une surverse avec une probabilité de 1/2 sans considérer aucune variable explicative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function getVarEx(date::Date)\n",
    "    row_sum = filter(row-> row.date== date, X_pcp_sum)\n",
    "    row_sum_2_days = filter(row-> row.date== date, X_pcp_two_days)\n",
    "    row_max = filter(row-> row.date== date, X_pcp_max)\n",
    "    row_max_3h = filter(row->row.date== date, X_max_3h)\n",
    "    row_bloc_3h = filter(row->row.date==date, X_bloc_3h)\n",
    "    total_row = join(row_sum, row_sum_2_days, on = :date)\n",
    "    total_row = join(total_row, row_max, on = :date)\n",
    "    total_row = join(total_row, row_max_3h, on = :date)\n",
    "    total_row = join(total_row, row_bloc_3h, on = :date)\n",
    "    \n",
    "    return total_row\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Pas classe, je sais\n",
    "function dfRowTo2D(row::DataFrameRow)\n",
    "    X = Array{Float64, 2}(undef, 1, length(row))\n",
    "    for i=1:length(row)\n",
    "        X[1, i] = row[i]\n",
    "    end\n",
    "    return X\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Chargement du fichier de test\n",
    "test = CSV.read(\"data/test.csv\")\n",
    "hcat(test, DataFrame(SURVERSE=zeros(Int64, length(test[:,1]))))\n",
    "rename!(test, :DATE => :date)\n",
    "\n",
    "## Dataframe des dates et ouvrages à prédire avec toutes les variables explicatives\n",
    "df_test = getVarEx(test[1, :date])\n",
    "for i=2:length(test[:,1])\n",
    "    push!(df_test, convert(Array, getVarEx(test[i, :date])))\n",
    "end\n",
    "insert!(df_test, 1, test[:, :NO_OUVRAGE], :NO_OUVRAGE)\n",
    "insert!(df_test, 3, zeros(Int64, length(df_test[:,1])), :surverse)\n",
    "#insert!(df_test, 4, ones(Float64, length(df_test[:,1])), :x1)\n",
    "ambig = []\n",
    "## Calcul de la probabilité de surverse\n",
    "for i=1:length(df_test[:,1])\n",
    "    M = best_models[df_test[i, :NO_OUVRAGE]]\n",
    "    Exp = dfRowTo2D(df_test[i,4:28])\n",
    "    proba = predict_proba(M, Exp)\n",
    "    prediction = predict(M, Exp)\n",
    "    ouvr = \"$(df_test[i, :NO_OUVRAGE])  on  $(df_test[i, :date])\"\n",
    "    if all(i -> i < 0.65, proba) #On print les jours ambiguës\n",
    "        #push!(ambig, (ouvr, proba))\n",
    "        println(ouvr)\n",
    "        println(proba)\n",
    "        println(prediction, \"\\n\")\n",
    "    end\n",
    "    \n",
    "    df_test[i, :surverse] = prediction[1]\n",
    "end\n",
    "#println(ambig)\n",
    "# Création du fichier sampleSubmission.csv pour soumettre sur Kaggle\n",
    "ID = test[:,:NO_OUVRAGE].*\"_\".*string.(test[:,:date])\n",
    "sampleSubmission = DataFrame(ID = ID, Surverse=df_test[:,:surverse])\n",
    "CSV.write(\"sampleSubmission_with_5_variables_rfpca_rand.csv\",sampleSubmission)\n",
    "\n",
    "# Vous pouvez par la suite déposer le fichier sampleSubmission.csv sur Kaggle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Julia 1.2.0",
   "language": "julia",
   "name": "julia-1.2"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
