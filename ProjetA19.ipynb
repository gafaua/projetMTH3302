{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# MTH3302 : Méthodes probabilistes et statistiques pour l'I.A.\n",
    "\n",
    "Jonathan Jalbert<br/>\n",
    "Professeur adjoint au Département de mathématiques et de génie industriel<br/>\n",
    "Polytechnique Montréal<br/>\n",
    "\n",
    "Le projet a été développé à l'aide de Alice Breton, étudiante à la maîtrise en génie informatique. Elle a suivi le cours lors de la session Hiver 2019.\n",
    "\n",
    "\n",
    "\n",
    "# Projet : Débordement d'égouts\n",
    "\n",
    "La description du projet est disponible à l'adresse suivante :\n",
    "https://www.kaggle.com/t/a238b752c33a41d9803c2cdde6bfc929\n",
    "\n",
    "Ce calepin Jupyter de base permet de charger et de nettoyer les données fournies. La dernière section détaille la génération du fichier des prédictions afin de le soumettre sur Kaggle dans le bon format.\n",
    "\n",
    "Dans un premier temps, vous devrez récupérer l'archive *data.zip* sur Moodle. Ce dossier contient les fichiers suivants :\n",
    "- surverses.csv\n",
    "- precipitation.csv\n",
    "- ouvrages-surverses.csv\n",
    "- test.csv\n",
    "\n",
    "Veuillez le décompresser dans le répertoire de ce calepin.\n",
    "\n",
    "Le fichier *surverse.csv* répertorie s'il y a surverse (1) ou non (0) au cours de la journée pour les 170 ouvrages de débordement de 2013 à 2018 pour les mois de mai à octobre (inclusivement). Des renseignements additionnels sur les données sont disponibles à l'adresse suivante :\n",
    "\n",
    "http://donnees.ville.montreal.qc.ca/dataset/debordement\n",
    "\n",
    "\n",
    "Le fichier *precipitation.csv* contient les précipitations horaires en dixième de *mm* enregistrées à 5 stations pluviométriques de 2013 à 2019 :\n",
    "- McTavish (7024745)\n",
    "- Ste-Anne-de-Bellevue (702FHL8)\n",
    "- Montreal/Pierre Elliott Trudeau Intl (702S006)\n",
    "- Montreal/St-Hubert (7027329)\n",
    "- L’Assomption (7014160)\n",
    "\n",
    "Plus d'informations sur les précipitations sont disponibles à l'adresse suivante :\n",
    "\n",
    "https://climat.meteo.gc.ca/climate_data/hourly_data_f.html?hlyRange=2008-01-08%7C2019-11-12&dlyRange=2002-12-23%7C2019-11-12&mlyRange=%7C&StationID=30165&Prov=QC&urlExtension=_f.html&searchType=stnName&optLimit=yearRange&StartYear=1840&EndYear=2019&selRowPerPage=25&Line=17&searchMethod=contains&Month=11&Day=12&txtStationName=montreal&timeframe=1&Year=2019\n",
    "\n",
    "Le fichier *ouvrages-surverses.csv* contient différentes caractéristiques des ouvrages de débordement. \n",
    "\n",
    "http://donnees.ville.montreal.qc.ca/dataset/ouvrage-surverse\n",
    "\n",
    "Le fichier *test.csv* contient les ouvrages et les jours pour lesquels vous devez prédire s'il y a eu surverse (true) ou non (false). Notez que l'on s'intéresse ici à 5 ouvrages de débordement localisés tout autour de l'Ile de Montréal :\n",
    "- 3260-01D dans Rivière-des-Prairies \n",
    "- 3350-07D dans Ahunstic \n",
    "- 4240-01D dans Pointe-aux-Trembles \n",
    "- 4350-01D dans le Vieux-Montréal \n",
    "- 4380-01D dans Verdun\n",
    "\n",
    "#### Remarque\n",
    "\n",
    "Dans le projet, on ne s'intéresse qu'aux surverses occasionnées par les précipitations. On ignore les surverses occasionnées par \n",
    "- fonte de neige (F)\n",
    "- travaux planifiés et entretien (TPL)\n",
    "- urgence (U)\n",
    "- autre (AUT)\n",
    "\n",
    "On suppose que lorsqu'il n'y a pas de raison pour la surverse, il s'agit d'une surverse causée par les précipitations. Puisque Nous nous intéresserons uniquement aux surverses occasionnées par les précipitations liquides, nous ne considérons que les mois de mai à octobre inclusivement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "using CSV, DataFrames, Statistics, Dates, Gadfly, Random, LinearAlgebra, GLM, Distributions, Combinatorics\n",
    "include(\"functions.jl\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chargement des données et nettoyage préliminaire"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chargement et nettoyage des surverses et précipitations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"data-frame\"><thead><tr><th></th><th>date</th><th>heure</th><th>McTavish</th><th>Bellevue</th><th>Assomption</th><th>Trudeau</th><th>StHubert</th></tr><tr><th></th><th>Date</th><th>Int64</th><th>Int64⍰</th><th>Int64⍰</th><th>Int64⍰</th><th>Int64⍰</th><th>Int64⍰</th></tr></thead><tbody><p>30,912 rows × 7 columns</p><tr><th>1</th><td>2013-05-01</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>missing</td></tr><tr><th>2</th><td>2013-05-01</td><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>missing</td></tr><tr><th>3</th><td>2013-05-01</td><td>2</td><td>0</td><td>0</td><td>0</td><td>0</td><td>missing</td></tr><tr><th>4</th><td>2013-05-01</td><td>3</td><td>0</td><td>0</td><td>0</td><td>0</td><td>missing</td></tr><tr><th>5</th><td>2013-05-01</td><td>4</td><td>0</td><td>0</td><td>0</td><td>0</td><td>missing</td></tr><tr><th>6</th><td>2013-05-01</td><td>5</td><td>0</td><td>0</td><td>0</td><td>0</td><td>missing</td></tr><tr><th>7</th><td>2013-05-01</td><td>6</td><td>0</td><td>0</td><td>0</td><td>0</td><td>missing</td></tr><tr><th>8</th><td>2013-05-01</td><td>7</td><td>0</td><td>0</td><td>0</td><td>0</td><td>missing</td></tr><tr><th>9</th><td>2013-05-01</td><td>8</td><td>0</td><td>0</td><td>0</td><td>0</td><td>missing</td></tr><tr><th>10</th><td>2013-05-01</td><td>9</td><td>0</td><td>0</td><td>0</td><td>0</td><td>missing</td></tr><tr><th>11</th><td>2013-05-01</td><td>10</td><td>0</td><td>0</td><td>0</td><td>0</td><td>missing</td></tr><tr><th>12</th><td>2013-05-01</td><td>11</td><td>0</td><td>0</td><td>0</td><td>0</td><td>missing</td></tr><tr><th>13</th><td>2013-05-01</td><td>12</td><td>0</td><td>0</td><td>0</td><td>0</td><td>missing</td></tr><tr><th>14</th><td>2013-05-01</td><td>13</td><td>0</td><td>0</td><td>0</td><td>0</td><td>missing</td></tr><tr><th>15</th><td>2013-05-01</td><td>14</td><td>0</td><td>0</td><td>0</td><td>0</td><td>missing</td></tr><tr><th>16</th><td>2013-05-01</td><td>15</td><td>0</td><td>0</td><td>0</td><td>0</td><td>missing</td></tr><tr><th>17</th><td>2013-05-01</td><td>16</td><td>0</td><td>0</td><td>0</td><td>0</td><td>missing</td></tr><tr><th>18</th><td>2013-05-01</td><td>17</td><td>0</td><td>0</td><td>0</td><td>0</td><td>missing</td></tr><tr><th>19</th><td>2013-05-01</td><td>18</td><td>0</td><td>0</td><td>0</td><td>0</td><td>missing</td></tr><tr><th>20</th><td>2013-05-01</td><td>19</td><td>0</td><td>0</td><td>0</td><td>0</td><td>missing</td></tr><tr><th>21</th><td>2013-05-01</td><td>20</td><td>0</td><td>0</td><td>0</td><td>0</td><td>missing</td></tr><tr><th>22</th><td>2013-05-01</td><td>21</td><td>0</td><td>0</td><td>0</td><td>0</td><td>missing</td></tr><tr><th>23</th><td>2013-05-01</td><td>22</td><td>0</td><td>0</td><td>0</td><td>0</td><td>missing</td></tr><tr><th>24</th><td>2013-05-01</td><td>23</td><td>0</td><td>0</td><td>0</td><td>0</td><td>missing</td></tr><tr><th>25</th><td>2013-05-02</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>missing</td></tr><tr><th>26</th><td>2013-05-02</td><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>missing</td></tr><tr><th>27</th><td>2013-05-02</td><td>2</td><td>0</td><td>0</td><td>0</td><td>0</td><td>missing</td></tr><tr><th>28</th><td>2013-05-02</td><td>3</td><td>0</td><td>0</td><td>0</td><td>0</td><td>missing</td></tr><tr><th>29</th><td>2013-05-02</td><td>4</td><td>0</td><td>0</td><td>0</td><td>0</td><td>missing</td></tr><tr><th>30</th><td>2013-05-02</td><td>5</td><td>0</td><td>0</td><td>0</td><td>0</td><td>missing</td></tr><tr><th>&vellip;</th><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td></tr></tbody></table>"
      ],
      "text/latex": [
       "\\begin{tabular}{r|ccccccc}\n",
       "\t& date & heure & McTavish & Bellevue & Assomption & Trudeau & StHubert\\\\\n",
       "\t\\hline\n",
       "\t& Date & Int64 & Int64⍰ & Int64⍰ & Int64⍰ & Int64⍰ & Int64⍰\\\\\n",
       "\t\\hline\n",
       "\t1 & 2013-05-01 & 0 & 0 & 0 & 0 & 0 &  \\\\\n",
       "\t2 & 2013-05-01 & 1 & 0 & 0 & 0 & 0 &  \\\\\n",
       "\t3 & 2013-05-01 & 2 & 0 & 0 & 0 & 0 &  \\\\\n",
       "\t4 & 2013-05-01 & 3 & 0 & 0 & 0 & 0 &  \\\\\n",
       "\t5 & 2013-05-01 & 4 & 0 & 0 & 0 & 0 &  \\\\\n",
       "\t6 & 2013-05-01 & 5 & 0 & 0 & 0 & 0 &  \\\\\n",
       "\t7 & 2013-05-01 & 6 & 0 & 0 & 0 & 0 &  \\\\\n",
       "\t8 & 2013-05-01 & 7 & 0 & 0 & 0 & 0 &  \\\\\n",
       "\t9 & 2013-05-01 & 8 & 0 & 0 & 0 & 0 &  \\\\\n",
       "\t10 & 2013-05-01 & 9 & 0 & 0 & 0 & 0 &  \\\\\n",
       "\t11 & 2013-05-01 & 10 & 0 & 0 & 0 & 0 &  \\\\\n",
       "\t12 & 2013-05-01 & 11 & 0 & 0 & 0 & 0 &  \\\\\n",
       "\t13 & 2013-05-01 & 12 & 0 & 0 & 0 & 0 &  \\\\\n",
       "\t14 & 2013-05-01 & 13 & 0 & 0 & 0 & 0 &  \\\\\n",
       "\t15 & 2013-05-01 & 14 & 0 & 0 & 0 & 0 &  \\\\\n",
       "\t16 & 2013-05-01 & 15 & 0 & 0 & 0 & 0 &  \\\\\n",
       "\t17 & 2013-05-01 & 16 & 0 & 0 & 0 & 0 &  \\\\\n",
       "\t18 & 2013-05-01 & 17 & 0 & 0 & 0 & 0 &  \\\\\n",
       "\t19 & 2013-05-01 & 18 & 0 & 0 & 0 & 0 &  \\\\\n",
       "\t20 & 2013-05-01 & 19 & 0 & 0 & 0 & 0 &  \\\\\n",
       "\t21 & 2013-05-01 & 20 & 0 & 0 & 0 & 0 &  \\\\\n",
       "\t22 & 2013-05-01 & 21 & 0 & 0 & 0 & 0 &  \\\\\n",
       "\t23 & 2013-05-01 & 22 & 0 & 0 & 0 & 0 &  \\\\\n",
       "\t24 & 2013-05-01 & 23 & 0 & 0 & 0 & 0 &  \\\\\n",
       "\t25 & 2013-05-02 & 0 & 0 & 0 & 0 & 0 &  \\\\\n",
       "\t26 & 2013-05-02 & 1 & 0 & 0 & 0 & 0 &  \\\\\n",
       "\t27 & 2013-05-02 & 2 & 0 & 0 & 0 & 0 &  \\\\\n",
       "\t28 & 2013-05-02 & 3 & 0 & 0 & 0 & 0 &  \\\\\n",
       "\t29 & 2013-05-02 & 4 & 0 & 0 & 0 & 0 &  \\\\\n",
       "\t30 & 2013-05-02 & 5 & 0 & 0 & 0 & 0 &  \\\\\n",
       "\t$\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "30912×7 DataFrame. Omitted printing of 1 columns\n",
       "│ Row   │ date       │ heure │ McTavish │ Bellevue │ Assomption │ Trudeau │\n",
       "│       │ \u001b[90mDate\u001b[39m       │ \u001b[90mInt64\u001b[39m │ \u001b[90mInt64⍰\u001b[39m   │ \u001b[90mInt64⍰\u001b[39m   │ \u001b[90mInt64⍰\u001b[39m     │ \u001b[90mInt64⍰\u001b[39m  │\n",
       "├───────┼────────────┼───────┼──────────┼──────────┼────────────┼─────────┤\n",
       "│ 1     │ 2013-05-01 │ 0     │ 0        │ 0        │ 0          │ 0       │\n",
       "│ 2     │ 2013-05-01 │ 1     │ 0        │ 0        │ 0          │ 0       │\n",
       "│ 3     │ 2013-05-01 │ 2     │ 0        │ 0        │ 0          │ 0       │\n",
       "│ 4     │ 2013-05-01 │ 3     │ 0        │ 0        │ 0          │ 0       │\n",
       "│ 5     │ 2013-05-01 │ 4     │ 0        │ 0        │ 0          │ 0       │\n",
       "│ 6     │ 2013-05-01 │ 5     │ 0        │ 0        │ 0          │ 0       │\n",
       "│ 7     │ 2013-05-01 │ 6     │ 0        │ 0        │ 0          │ 0       │\n",
       "│ 8     │ 2013-05-01 │ 7     │ 0        │ 0        │ 0          │ 0       │\n",
       "│ 9     │ 2013-05-01 │ 8     │ 0        │ 0        │ 0          │ 0       │\n",
       "│ 10    │ 2013-05-01 │ 9     │ 0        │ 0        │ 0          │ 0       │\n",
       "⋮\n",
       "│ 30902 │ 2019-10-31 │ 13    │ \u001b[90mmissing\u001b[39m  │ \u001b[90mmissing\u001b[39m  │ \u001b[90mmissing\u001b[39m    │ \u001b[90mmissing\u001b[39m │\n",
       "│ 30903 │ 2019-10-31 │ 14    │ \u001b[90mmissing\u001b[39m  │ \u001b[90mmissing\u001b[39m  │ \u001b[90mmissing\u001b[39m    │ \u001b[90mmissing\u001b[39m │\n",
       "│ 30904 │ 2019-10-31 │ 15    │ \u001b[90mmissing\u001b[39m  │ \u001b[90mmissing\u001b[39m  │ \u001b[90mmissing\u001b[39m    │ \u001b[90mmissing\u001b[39m │\n",
       "│ 30905 │ 2019-10-31 │ 16    │ \u001b[90mmissing\u001b[39m  │ \u001b[90mmissing\u001b[39m  │ \u001b[90mmissing\u001b[39m    │ \u001b[90mmissing\u001b[39m │\n",
       "│ 30906 │ 2019-10-31 │ 17    │ \u001b[90mmissing\u001b[39m  │ \u001b[90mmissing\u001b[39m  │ \u001b[90mmissing\u001b[39m    │ \u001b[90mmissing\u001b[39m │\n",
       "│ 30907 │ 2019-10-31 │ 18    │ \u001b[90mmissing\u001b[39m  │ \u001b[90mmissing\u001b[39m  │ \u001b[90mmissing\u001b[39m    │ \u001b[90mmissing\u001b[39m │\n",
       "│ 30908 │ 2019-10-31 │ 19    │ \u001b[90mmissing\u001b[39m  │ \u001b[90mmissing\u001b[39m  │ \u001b[90mmissing\u001b[39m    │ \u001b[90mmissing\u001b[39m │\n",
       "│ 30909 │ 2019-10-31 │ 20    │ \u001b[90mmissing\u001b[39m  │ \u001b[90mmissing\u001b[39m  │ \u001b[90mmissing\u001b[39m    │ \u001b[90mmissing\u001b[39m │\n",
       "│ 30910 │ 2019-10-31 │ 21    │ \u001b[90mmissing\u001b[39m  │ \u001b[90mmissing\u001b[39m  │ \u001b[90mmissing\u001b[39m    │ \u001b[90mmissing\u001b[39m │\n",
       "│ 30911 │ 2019-10-31 │ 22    │ \u001b[90mmissing\u001b[39m  │ \u001b[90mmissing\u001b[39m  │ \u001b[90mmissing\u001b[39m    │ \u001b[90mmissing\u001b[39m │\n",
       "│ 30912 │ 2019-10-31 │ 23    │ \u001b[90mmissing\u001b[39m  │ \u001b[90mmissing\u001b[39m  │ \u001b[90mmissing\u001b[39m    │ \u001b[90mmissing\u001b[39m │"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = CSV.read(\"data/surverses.csv\",missingstring=\"-99999\")\n",
    "first(data,5)\n",
    "\n",
    "# Extraction des surverses pour les mois de mai à octobre inclusivement\n",
    "data = filter(row -> month(row.DATE) > 4, data) \n",
    "data = filter(row -> month(row.DATE) < 11, data) \n",
    "\n",
    "# Remplacement des valeurs *missing* dans la colonne :RAISON par \"Inconnue\"\n",
    "raison = coalesce.(data[:,:RAISON],\"Inconnue\")\n",
    "data[!,:RAISON] = raison\n",
    "\n",
    "# Exlusion des surverses coccasionnées par d'autres facteurs que les précipitations liquides\n",
    "data = filter(row -> row.RAISON ∈ [\"P\",\"Inconnue\",\"TS\"], data) \n",
    "select!(data, [:NO_OUVRAGE, :DATE, :SURVERSE])\n",
    "\n",
    "surverse_df = dropmissing(data, disallowmissing=true)\n",
    "\n",
    "### CHARGEMENT DES PRÉCIPITATIONS\n",
    "data = CSV.read(\"data/precipitations.csv\",missingstring=\"-99999\")\n",
    "rename!(data, Symbol(\"St-Hubert\")=>:StHubert)\n",
    "\n",
    "# Nettoyage des données sur les précipitations\n",
    "data = filter(row -> month(row.date) > 4, data) \n",
    "data = filter(row -> month(row.date) < 11, data) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"data-frame\"><thead><tr><th></th><th>date</th><th>heure</th><th>McTavish</th><th>Bellevue</th><th>Assomption</th><th>Trudeau</th><th>StHubert</th></tr><tr><th></th><th>Date</th><th>Int64</th><th>Int64</th><th>Int64</th><th>Int64</th><th>Int64</th><th>Int64</th></tr></thead><tbody><p>30,864 rows × 7 columns</p><tr><th>1</th><td>2013-05-01</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><th>2</th><td>2013-05-01</td><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><th>3</th><td>2013-05-01</td><td>2</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><th>4</th><td>2013-05-01</td><td>3</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><th>5</th><td>2013-05-01</td><td>4</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><th>6</th><td>2013-05-01</td><td>5</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><th>7</th><td>2013-05-01</td><td>6</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><th>8</th><td>2013-05-01</td><td>7</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><th>9</th><td>2013-05-01</td><td>8</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><th>10</th><td>2013-05-01</td><td>9</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><th>11</th><td>2013-05-01</td><td>10</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><th>12</th><td>2013-05-01</td><td>11</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><th>13</th><td>2013-05-01</td><td>12</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><th>14</th><td>2013-05-01</td><td>13</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><th>15</th><td>2013-05-01</td><td>14</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><th>16</th><td>2013-05-01</td><td>15</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><th>17</th><td>2013-05-01</td><td>16</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><th>18</th><td>2013-05-01</td><td>17</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><th>19</th><td>2013-05-01</td><td>18</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><th>20</th><td>2013-05-01</td><td>19</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><th>21</th><td>2013-05-01</td><td>20</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><th>22</th><td>2013-05-01</td><td>21</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><th>23</th><td>2013-05-01</td><td>22</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><th>24</th><td>2013-05-01</td><td>23</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><th>25</th><td>2013-05-02</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><th>26</th><td>2013-05-02</td><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><th>27</th><td>2013-05-02</td><td>2</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><th>28</th><td>2013-05-02</td><td>3</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><th>29</th><td>2013-05-02</td><td>4</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><th>30</th><td>2013-05-02</td><td>5</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><th>&vellip;</th><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td></tr></tbody></table>"
      ],
      "text/latex": [
       "\\begin{tabular}{r|ccccccc}\n",
       "\t& date & heure & McTavish & Bellevue & Assomption & Trudeau & StHubert\\\\\n",
       "\t\\hline\n",
       "\t& Date & Int64 & Int64 & Int64 & Int64 & Int64 & Int64\\\\\n",
       "\t\\hline\n",
       "\t1 & 2013-05-01 & 0 & 0 & 0 & 0 & 0 & 0 \\\\\n",
       "\t2 & 2013-05-01 & 1 & 0 & 0 & 0 & 0 & 0 \\\\\n",
       "\t3 & 2013-05-01 & 2 & 0 & 0 & 0 & 0 & 0 \\\\\n",
       "\t4 & 2013-05-01 & 3 & 0 & 0 & 0 & 0 & 0 \\\\\n",
       "\t5 & 2013-05-01 & 4 & 0 & 0 & 0 & 0 & 0 \\\\\n",
       "\t6 & 2013-05-01 & 5 & 0 & 0 & 0 & 0 & 0 \\\\\n",
       "\t7 & 2013-05-01 & 6 & 0 & 0 & 0 & 0 & 0 \\\\\n",
       "\t8 & 2013-05-01 & 7 & 0 & 0 & 0 & 0 & 0 \\\\\n",
       "\t9 & 2013-05-01 & 8 & 0 & 0 & 0 & 0 & 0 \\\\\n",
       "\t10 & 2013-05-01 & 9 & 0 & 0 & 0 & 0 & 0 \\\\\n",
       "\t11 & 2013-05-01 & 10 & 0 & 0 & 0 & 0 & 0 \\\\\n",
       "\t12 & 2013-05-01 & 11 & 0 & 0 & 0 & 0 & 0 \\\\\n",
       "\t13 & 2013-05-01 & 12 & 0 & 0 & 0 & 0 & 0 \\\\\n",
       "\t14 & 2013-05-01 & 13 & 0 & 0 & 0 & 0 & 0 \\\\\n",
       "\t15 & 2013-05-01 & 14 & 0 & 0 & 0 & 0 & 0 \\\\\n",
       "\t16 & 2013-05-01 & 15 & 0 & 0 & 0 & 0 & 0 \\\\\n",
       "\t17 & 2013-05-01 & 16 & 0 & 0 & 0 & 0 & 0 \\\\\n",
       "\t18 & 2013-05-01 & 17 & 0 & 0 & 0 & 0 & 0 \\\\\n",
       "\t19 & 2013-05-01 & 18 & 0 & 0 & 0 & 0 & 0 \\\\\n",
       "\t20 & 2013-05-01 & 19 & 0 & 0 & 0 & 0 & 0 \\\\\n",
       "\t21 & 2013-05-01 & 20 & 0 & 0 & 0 & 0 & 0 \\\\\n",
       "\t22 & 2013-05-01 & 21 & 0 & 0 & 0 & 0 & 0 \\\\\n",
       "\t23 & 2013-05-01 & 22 & 0 & 0 & 0 & 0 & 0 \\\\\n",
       "\t24 & 2013-05-01 & 23 & 0 & 0 & 0 & 0 & 0 \\\\\n",
       "\t25 & 2013-05-02 & 0 & 0 & 0 & 0 & 0 & 0 \\\\\n",
       "\t26 & 2013-05-02 & 1 & 0 & 0 & 0 & 0 & 0 \\\\\n",
       "\t27 & 2013-05-02 & 2 & 0 & 0 & 0 & 0 & 0 \\\\\n",
       "\t28 & 2013-05-02 & 3 & 0 & 0 & 0 & 0 & 0 \\\\\n",
       "\t29 & 2013-05-02 & 4 & 0 & 0 & 0 & 0 & 0 \\\\\n",
       "\t30 & 2013-05-02 & 5 & 0 & 0 & 0 & 0 & 0 \\\\\n",
       "\t$\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "30864×7 DataFrame. Omitted printing of 1 columns\n",
       "│ Row   │ date       │ heure │ McTavish │ Bellevue │ Assomption │ Trudeau │\n",
       "│       │ \u001b[90mDate\u001b[39m       │ \u001b[90mInt64\u001b[39m │ \u001b[90mInt64\u001b[39m    │ \u001b[90mInt64\u001b[39m    │ \u001b[90mInt64\u001b[39m      │ \u001b[90mInt64\u001b[39m   │\n",
       "├───────┼────────────┼───────┼──────────┼──────────┼────────────┼─────────┤\n",
       "│ 1     │ 2013-05-01 │ 0     │ 0        │ 0        │ 0          │ 0       │\n",
       "│ 2     │ 2013-05-01 │ 1     │ 0        │ 0        │ 0          │ 0       │\n",
       "│ 3     │ 2013-05-01 │ 2     │ 0        │ 0        │ 0          │ 0       │\n",
       "│ 4     │ 2013-05-01 │ 3     │ 0        │ 0        │ 0          │ 0       │\n",
       "│ 5     │ 2013-05-01 │ 4     │ 0        │ 0        │ 0          │ 0       │\n",
       "│ 6     │ 2013-05-01 │ 5     │ 0        │ 0        │ 0          │ 0       │\n",
       "│ 7     │ 2013-05-01 │ 6     │ 0        │ 0        │ 0          │ 0       │\n",
       "│ 8     │ 2013-05-01 │ 7     │ 0        │ 0        │ 0          │ 0       │\n",
       "│ 9     │ 2013-05-01 │ 8     │ 0        │ 0        │ 0          │ 0       │\n",
       "│ 10    │ 2013-05-01 │ 9     │ 0        │ 0        │ 0          │ 0       │\n",
       "⋮\n",
       "│ 30854 │ 2019-10-29 │ 14    │ 0        │ 0        │ 0          │ 0       │\n",
       "│ 30855 │ 2019-10-29 │ 15    │ 0        │ 0        │ 0          │ 0       │\n",
       "│ 30856 │ 2019-10-29 │ 16    │ 0        │ 0        │ 0          │ 0       │\n",
       "│ 30857 │ 2019-10-29 │ 17    │ 0        │ 0        │ 0          │ 0       │\n",
       "│ 30858 │ 2019-10-29 │ 18    │ 0        │ 0        │ 0          │ 0       │\n",
       "│ 30859 │ 2019-10-29 │ 19    │ 0        │ 0        │ 0          │ 0       │\n",
       "│ 30860 │ 2019-10-29 │ 20    │ 0        │ 0        │ 0          │ 0       │\n",
       "│ 30861 │ 2019-10-29 │ 21    │ 0        │ 0        │ 0          │ 0       │\n",
       "│ 30862 │ 2019-10-29 │ 22    │ 0        │ 0        │ 0          │ 0       │\n",
       "│ 30863 │ 2019-10-29 │ 23    │ 0        │ 0        │ 0          │ 0       │\n",
       "│ 30864 │ 2019-10-30 │ 0     │ 0        │ 0        │ 0          │ 0       │"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# REMPLACER LES VALEURS MANQUANTES DANS LE DATAFRAME DE PRÉCIPITATIONS\n",
    "# On veut remplacer les données manquante grâce à une simple régression linéaire (Ridge).\n",
    "# Pour ce faire, chaque station météo agira comme variable explicative pour les 4 autres\n",
    "# On drop les rangées dont les 5valeurs sont missing\n",
    "\n",
    "data_full = deepcopy(data)\n",
    "dropmissing!(data, disallowmissing=true)\n",
    "\n",
    "models = Dict()\n",
    "\n",
    "for i = 1:length(data_full[:, 1])\n",
    "    if any(ismissing, data_full[i, 3:7]) #ismissing c est pas une variable mais une méthode de Julia, 3:7 pour les 5 stations\n",
    "        missing = []\n",
    "        notmissing = []\n",
    "        for j=3:7 #on passe à travers les 5 colonnes pour regarder celles qui sont missing ou non\n",
    "            if ismissing(data_full[i, j])\n",
    "                push!(missing, names(data_full)[j]) #push généralisé à tous les conteneurs\n",
    "                #Le point d exclamation pour dire à la fonction de modifier le conteneur qu on lui passe\n",
    "            else\n",
    "                push!(notmissing, names(data_full)[j])\n",
    "            end\n",
    "        end\n",
    "        \n",
    "        if length(notmissing) > 0 #si on a 5 missing sur la ligne on peut juste rien prédire donc on skip ça\n",
    "            for m in missing #m c est la station à prédire\n",
    "                if !haskey(models, (m, notmissing)) #ça véfirie si dans le dictionnaire models il y a la key (m, notmissing)\n",
    "                    models[(m, notmissing)] = CoeffRidge(data, m, notmissing) #on change la case dans le dico palr le coef que la fonction nous retourne\n",
    "                end\n",
    "                data_full[i, m] = round(convert(Vector{Float64}, data_full[i, notmissing])' * models[(m, notmissing)]) # on ajoute à la case manque la nouvelle valeur = x*Beta\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "end\n",
    "\n",
    "dropmissing!(data_full)\n",
    "data_full"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extraction variables explicatives (Maude L.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maximum de précipitations en 1 heure dans une journée.\n",
    "X_pcp_max = by(data_full, :date,  McTavish = :McTavish=>maximum, Bellevue = :Bellevue=>maximum, \n",
    "   Assomption = :Assomption=>maximum, Trudeau = :Trudeau=>maximum, StHubert = :StHubert=>maximum)\n",
    "\n",
    "# Somme des précipitations dans une journée\n",
    "X_pcp_sum = by(data_full, :date,  McTavish = :McTavish=>sum, Bellevue = :Bellevue=>sum, \n",
    "   Assomption = :Assomption=>sum, Trudeau = :Trudeau=>sum, StHubert = :StHubert=>sum)\n",
    "\n",
    "# Précipitation à chaque heure pour chaque journée\n",
    "X_pcp_hour = data_full\n",
    "\n",
    "# Somme des précipitations des 2 heures précédentes\n",
    "\n",
    "X_pcp_three_hours = copy(data_full)\n",
    "buffer = copy(data_full)\n",
    "for i=3:7\n",
    "    for j=1:length(data_full[:,2])\n",
    "        if (j > 2)\n",
    "            X_pcp_three_hours[j, i] = (buffer[(j-2), i] + buffer[(j-1), i] + buffer[j,i])\n",
    "        else\n",
    "            X_pcp_three_hours[j, i] = buffer[j, i]\n",
    "        end\n",
    "    end\n",
    "end\n",
    "\n",
    "# Somme des précipitations des 2 jours précédents\n",
    "\n",
    "X_pcp_two_days = copy(X_pcp_sum)\n",
    "buffer = copy(X_pcp_sum)\n",
    "for i=2:6\n",
    "    for j=1:length(X_pcp_sum[:,2])\n",
    "        if (j > 1)\n",
    "            X_pcp_two_days[j, i] = buffer[(j-1), i] + buffer[j,i]\n",
    "        else\n",
    "            X_pcp_two_days[j, i] = buffer[j, i]\n",
    "        end\n",
    "    end\n",
    "end\n",
    "\n",
    "\n",
    "# Mois de la prédiction pour chaque ouvrage\n",
    "X_date = DataFrame(NO_OUVRAGE=surverse_df[:,:NO_OUVRAGE], MOIS=zeros(Int64,length(surverse_df[:,1])), SURVERSE=surverse_df[:,:SURVERSE])\n",
    "for i=1:length(surverse_df[:,1])\n",
    "    X_date[i,:MOIS] = month(surverse_df[i,:DATE])\n",
    "end\n",
    "\n",
    "X_mois = DataFrame(MOIS=[\"Mai\", \"Juin\", \"Juillet\", \"Août\", \"Septembre\", \"Octobre\"], RivierePrairie=zeros(Int64,6), Ahuntsic=zeros(Int64,6), PointeAuxTrembles=zeros(Int64,6), VieuxMontreal=zeros(Int64,6), Verdun=zeros(Int64,6))\n",
    "for j=1:length(X_date[:,1])\n",
    "    if(X_date[j,1] == \"3260-01D\")\n",
    "        X_mois[(X_date[j,2]-4),:RivierePrairie] += (X_date[j,3])\n",
    "    elseif (X_date[j,1] == \"3350-07D\")\n",
    "        X_mois[(X_date[j,2]-4),:Ahuntsic] += (X_date[j,3])\n",
    "    elseif (X_date[j,1] == \"4240-01D\")\n",
    "        X_mois[(X_date[j,2]-4),:PointeAuxTrembles] += (X_date[j,3])\n",
    "    elseif (X_date[j,1] == \"4350-01D\")\n",
    "        X_mois[(X_date[j,2]-4),:VieuxMontreal] += (X_date[j,3])\n",
    "    elseif (X_date[j,1] == \"4380-01D\")\n",
    "        X_mois[(X_date[j,2]-4),:Verdun] += (X_date[j,3])\n",
    "    end\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création de 5 dataframes séparés pour les données de surverses des 5 ouvrages d'intérêt\n",
    "ouvrage = [\"3260-01D\", \"3350-07D\", \"4240-01D\", \"4350-01D\", \"4380-01D\"]\n",
    "\n",
    "filtered_surverse = filter(row -> row.NO_OUVRAGE == ouvrage[1], surverse_df)\n",
    "surverse_rivierePrairie = DataFrame(DATE=filtered_surverse[:, 2], RIVIEREPRAIRIE=filtered_surverse[:, 3])\n",
    "\n",
    "filtered_surverse = filter(row -> row.NO_OUVRAGE == ouvrage[2], surverse_df)\n",
    "surverse_ahuntsic = DataFrame(DATE=filtered_surverse[:, 2], AHUNTSIC=filtered_surverse[:, 3])\n",
    "\n",
    "filtered_surverse = filter(row -> row.NO_OUVRAGE == ouvrage[3], surverse_df)\n",
    "surverse_pointeAuxTrembles = DataFrame(DATE=filtered_surverse[:, 2], POINTEAUXTREMBLES=filtered_surverse[:, 3])\n",
    "\n",
    "filtered_surverse = filter(row -> row.NO_OUVRAGE == ouvrage[4], surverse_df)\n",
    "surverse_vieuxMontreal = DataFrame(DATE=filtered_surverse[:, 2], VIEUXMONTREAL=filtered_surverse[:, 3])\n",
    "\n",
    "filtered_surverse = filter(row -> row.NO_OUVRAGE == ouvrage[5], surverse_df)\n",
    "surverse_verdun = DataFrame(DATE=filtered_surverse[:, 2], VERDUN=filtered_surverse[:, 3]);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyse exploratoire\n",
    "\n",
    "Cette section consitue une analyse exploratoire superficielle permettant de voir s'il existe un lien entre les précipitations et les surverses.\n",
    "\n",
    "Prenons arbitrairement l'ouvrage de débordement près du Bota-Bota (4350-01D). La station météorologique la plus proche est McTavish. Prenons deux variables explicatives simple :\n",
    "- la somme journalière des précipitations\n",
    "- le taux horaire maximum journalier de précipitations\n",
    "\n",
    "#### Calcul de la quantité journalière de précipitations pour chacune des stations météorologiques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "UndefVarError",
     "evalue": "UndefVarError: df not defined",
     "output_type": "error",
     "traceback": [
      "UndefVarError: df not defined",
      "",
      "Stacktrace:",
      " [1] top-level scope at In[6]:1"
     ]
    }
   ],
   "source": [
    "plot(df, x=:SURVERSE, y=:SUM, Geom.boxplot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modèle #1: Régression logistique "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Écrire une description "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CreateDataframe (generic function with 1 method)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Construction d'un dataframe contenant toutes les variables explicatives\n",
    "#ouvrage = \"3260-01D\"\n",
    "\n",
    "function CreateDataframe(ouvrage, surverse_df)\n",
    "    df_train = filter(row -> row.NO_OUVRAGE == ouvrage, surverse_df)\n",
    "\n",
    "    x₁ = Array{Int64}(undef, size(df_train,1)) # variable pour la somme journalière McTavish\n",
    "    x₂ = Array{Int64}(undef, size(df_train,1)) # variable pour la somme journalière Bellevue\n",
    "    x₃ = Array{Int64}(undef, size(df_train,1)) # variable pour la somme journalière Assomption\n",
    "    x₄ = Array{Int64}(undef, size(df_train,1)) # variable pour la somme journalière St-Hubert\n",
    "    x₅ = Array{Int64}(undef, size(df_train,1)) # variable pour la somme journalière PET\n",
    "    x₆ = Array{Int64}(undef, size(df_train,1)) # variable pour la somme des 2 jours McTavish\n",
    "    x₇ = Array{Int64}(undef, size(df_train,1)) # variable pour la somme des 2 jours Bellevue\n",
    "    x₈ = Array{Int64}(undef, size(df_train,1)) # variable pour la somme des 2 jours Assomption\n",
    "    x₉ = Array{Int64}(undef, size(df_train,1)) # variable pour la somme des 2 jours St-Hubert\n",
    "    x₁₀ = Array{Int64}(undef, size(df_train,1)) # variable pour la somme des 2 jours PET\n",
    "    x₁₁ = Array{Int64}(undef, size(df_train,1)) # variable pour le max journalier McTavish\n",
    "    x₁₂ = Array{Int64}(undef, size(df_train,1)) # variable pour le max journalier Bellevue\n",
    "    x₁₃ = Array{Int64}(undef, size(df_train,1)) # variable pour le max journalier Assomption\n",
    "    x₁₄ = Array{Int64}(undef, size(df_train,1)) # variable pour le max journalier St-Hubert\n",
    "    x₁₅ = Array{Int64}(undef, size(df_train,1)) # variable pour le max journalier PET\n",
    "\n",
    "\n",
    "\n",
    "    for i=1:size(df_train,1)\n",
    "\n",
    "        ind = findfirst(X_pcp_sum[:,:date] .== df_train[i,:DATE])\n",
    "        \n",
    "        x₁[i] = X_pcp_sum[ind,:McTavish]\n",
    "\n",
    "        x₂[i] = X_pcp_sum[ind,:Bellevue]\n",
    "\n",
    "        x₃[i] = X_pcp_sum[ind,:Assomption]\n",
    "\n",
    "        x₄[i] = X_pcp_sum[ind,:StHubert]\n",
    "\n",
    "        x₅[i] = X_pcp_sum[ind,:Trudeau]\n",
    "\n",
    "        x₆[i] = X_pcp_two_days[ind, :McTavish]\n",
    "\n",
    "        x₇[i] = X_pcp_two_days[ind, :Bellevue]\n",
    "\n",
    "        x₈[i] = X_pcp_two_days[ind, :Assomption]\n",
    "\n",
    "        x₉[i] = X_pcp_two_days[ind, :StHubert]\n",
    "\n",
    "        x₁₀[i] = X_pcp_two_days[ind, :Trudeau]\n",
    "\n",
    "        x₁₁[i] = X_pcp_max[ind, :McTavish]\n",
    "\n",
    "        x₁₂[i] = X_pcp_max[ind, :Bellevue]\n",
    "\n",
    "        x₁₃[i] = X_pcp_max[ind, :Assomption]\n",
    "\n",
    "        x₁₄[i] = X_pcp_max[ind, :StHubert]\n",
    "\n",
    "        x₁₅[i] = X_pcp_max[ind, :Trudeau]\n",
    "\n",
    "    end\n",
    "\n",
    "    df_train[!,:SUMMcTavish] = x₁\n",
    "    df_train[!,:SUMBellevue] = x₂\n",
    "    df_train[!,:SUMAssomption] = x₃\n",
    "    df_train[!,:SUMStHubert] = x₄\n",
    "    df_train[!,:SUMPET] = x₅\n",
    "    df_train[!,:SUM2McTavish] = x₆\n",
    "    df_train[!,:SUM2Bellevue] = x₇\n",
    "    df_train[!,:SUM2Assomption] = x₈\n",
    "    df_train[!,:SUM2StHubert] = x₉\n",
    "    df_train[!,:SUM2PET] = x₁₀\n",
    "    df_train[!,:MAXMcTavish] = x₁₁\n",
    "    df_train[!,:MAXBellevue] = x₁₂\n",
    "    df_train[!,:MAXAssomption] = x₁₃\n",
    "    df_train[!,:MAXStHubert] = x₁₄\n",
    "    df_train[!,:MAXPET] = x₁₅\n",
    "\n",
    "    \n",
    "    return df_train\n",
    "\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TrainLogit (generic function with 4 methods)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function TrainLogit(df::DataFrame, var::Matrix{Float64})\n",
    "#     X_var = df[:, filter(x -> (x in var), names(df))]\n",
    "    X = hcat(ones(Float64, length(df[:, 1])), var)\n",
    "    M = fit(GeneralizedLinearModel, X, df[:, :SURVERSE], Bernoulli(), LogitLink())\n",
    "    \n",
    "    return M\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BestLogitModel (generic function with 2 methods)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function BestLogitModel()\n",
    "    train, test = splitdataframe(CreateDataframe(\"3350-07D\", surverse_df), 0.8)\n",
    "    models = DataFrame(VARS=Array{Int64, 1}[], F₁=Float64[])\n",
    "    \n",
    "    for i=1:15\n",
    "        comb = collect(combinations(collect(4:18), i))\n",
    "        for columns in comb\n",
    "            X_train = convert(Matrix{Float64}, train[:, columns])\n",
    "            M = TrainLogit(train, X_train)\n",
    "            X_test = hcat(ones(Float64, length(test[:, 1])), convert(Matrix{Float64}, test[:, columns]))\n",
    "            θ = predict(M, X_test)\n",
    "            Y = θ .> 0.5\n",
    "            F₁ = computeF1score(convert(Array{Int64}, Y), test[:, :SURVERSE])\n",
    "            push!(models, [columns, F₁])\n",
    "        end\n",
    "    end\n",
    "    _, bestM = findmax(models[:, :F₁])\n",
    "    return models[bestM, :]    \n",
    "end "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected columns: [6, 10, 11, 14, 16, 17]\n",
      "F1: 0.8301886792452831\n"
     ]
    }
   ],
   "source": [
    "bestModel = BestLogitModel()\n",
    "println(\"Selected columns: $(bestModel[:VARS])\")\n",
    "println(\"F1: $(bestModel[:F₁])\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GeneralizedLinearModel{GLM.GlmResp{Array{Float64,1},Bernoulli{Float64},LogitLink},GLM.DensePredChol{Float64,Cholesky{Float64,Array{Float64,2}}}}:\n",
       "\n",
       "Coefficients:\n",
       "────────────────────────────────────────────────────────────────────────\n",
       "      Estimate  Std. Error    z value  Pr(>|z|)    Lower 95%   Upper 95%\n",
       "────────────────────────────────────────────────────────────────────────\n",
       "x1  -3.28187    0.19674     -16.6813     <1e-61  -3.66748     -2.89627  \n",
       "x2   0.0117576  0.00139124    8.45116    <1e-16   0.00903082   0.0144844\n",
       "────────────────────────────────────────────────────────────────────────\n"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_3260, df_test_3260 = splitdataframe(CreateDataframe(\"3260-01D\", surverse_df), 0.7) \n",
    "df_train_3350, df_test_3350 = splitdataframe(CreateDataframe(\"3350-07D\", surverse_df), 0.7)\n",
    "df_train_4240, df_test_4240 = splitdataframe(CreateDataframe(\"4240-01D\", surverse_df), 0.7)\n",
    "df_train_4350, df_test_4350 = splitdataframe(CreateDataframe(\"4350-01D\", surverse_df), 0.7)\n",
    "df_train_4380, df_test_4380 = splitdataframe(CreateDataframe(\"4380-01D\", surverse_df), 0.7)\n",
    "\n",
    "M_3260 = TrainLogit(df_train_3260, [Symbol(\"SUMMcTavish\"), Symbol(\"SUMBellevue\"), Symbol(\"SUMAssomption\"), Symbol(\"SUMStHubert\"), Symbol(\"SUMPET\"), Symbol(\"SUM2McTavish\"), Symbol(\"SUM2Bellevue\"), Symbol(\"SUM2Assomption\"), Symbol(\"SUM2StHubert\"), Symbol(\"SUM2PET\"), Symbol(\"MAXMcTavish\"), Symbol(\"\")])\n",
    "M_3350 = TrainLogit(df_train_3350, ) \n",
    "M_4240 = TrainLogit(df_train_4240, )\n",
    "M_4350 = TrainLogit(df_train_4350, )\n",
    "M_4380 = TrainLogit(df_train_4380, )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Création du fichier de prédictions pour soumettre sur Kaggle\n",
    "\n",
    "Dans ce cas-ci, nous prédirons une surverse avec une probabilité de 1/2 sans considérer aucune variable explicative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"sampleSubmission_with_zero.csv\""
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Chargement du fichier de test\n",
    "test = CSV.read(\"data/test.csv\")\n",
    "hcat(test, DataFrame(SURVERSE=zeros(Int64, length(test[:,1]))))\n",
    "\n",
    "df_test = DataFrame(NO_OUVRAGE=test[:, :NO_OUVRAGE], DATE=test[:, :DATE], SUMMcTavish=zeros(Int64, length(test[:,1])),SUMBellevue=zeros(Int64, length(test[:,1])),SUMAssomption=zeros(Int64, length(test[:,1])),SUMStHubert=zeros(Int64, length(test[:,1])),SUMPET=zeros(Int64, length(test[:,1])),SUM2McTavish=zeros(Int64, length(test[:,1])),SUM2Bellevue=zeros(Int64, length(test[:,1])),SUM2Assomption=zeros(Int64, length(test[:,1])),SUM2StHubert=zeros(Int64, length(test[:,1])),SUM2PET=zeros(Int64, length(test[:,1])),MAXMcTavish=zeros(Int64, length(test[:,1])),MAXBellevue=zeros(Int64, length(test[:,1])),MAXAssomption=zeros(Int64, length(test[:,1])),MAXStHubert=zeros(Int64, length(test[:,1])),MAXPET=zeros(Int64, length(test[:,1])), SURVERSE=zeros(Float64, length(test[:,1])))\n",
    "for i=1:length(df_test[:,1])\n",
    "    if df_test[i, :DATE] in dates_with_zero_mm\n",
    "        continue\n",
    "    end\n",
    "    row_sum = filter(row-> row.date== df_test[i,2], X_pcp_sum)\n",
    "    row_sum_2_days = filter(row-> row.date== df_test[i,2], X_pcp_two_days)\n",
    "    row_max = filter(row-> row.date== df_test[i,2], X_pcp_max)\n",
    "    for j=2:6\n",
    "        df_test[i, j+1] = row_sum[1, j]\n",
    "    end\n",
    "    for j=7:11\n",
    "        df_test[i, j+1] = row_sum_2_days[1, j-5]\n",
    "    end\n",
    "    for j=12:16\n",
    "        df_test[i, j+1] = row_max[1, j-10]\n",
    "    end\n",
    "end\n",
    "\n",
    "\n",
    "M_3260 = TrainLogit(\"3260-01D\", surverse_df)\n",
    "M_3350 = TrainLogit(\"3350-07D\", surverse_df)\n",
    "M_4240 = TrainLogit(\"4240-01D\", surverse_df)\n",
    "M_4350 = TrainLogit(\"4350-01D\", surverse_df)\n",
    "M_4380 = TrainLogit(\"4380-01D\", surverse_df)\n",
    "\n",
    "df_3260 = filter(row -> row.NO_OUVRAGE == \"3260-01D\", df_test)\n",
    "df_3350 = filter(row -> row.NO_OUVRAGE == \"3350-07D\", df_test)\n",
    "df_4240 = filter(row -> row.NO_OUVRAGE == \"4240-01D\", df_test)\n",
    "df_4350 = filter(row -> row.NO_OUVRAGE == \"4350-01D\", df_test)\n",
    "df_4380 = filter(row -> row.NO_OUVRAGE == \"4380-01D\", df_test)\n",
    "\n",
    "\n",
    "# Estimation de la probabilité de surverse de l'ensemble de test\n",
    "X̃ = hcat(ones(Float64,length(df_test[:,1])),df_test[:,:SUMMcTavish], df_test[:,:SUMBellevue], df_test[:,:SUMAssomption], df_test[:,:SUMStHubert], df_test[:,:SUMPET], df_test[:, :SUM2McTavish], df_test[:, :SUM2Bellevue], df_test[:, :SUM2Assomption], df_test[:, :SUM2StHubert], df_test[:, :SUM2PET], df_test[:, :MAXMcTavish], df_test[:, :MAXBellevue], df_test[:, :MAXAssomption], df_test[:, :MAXStHubert], df_test[:, :MAXPET])\n",
    "X̃_3260 = hcat(ones(Float64,length(df_3260[:,1])),df_3260[:,:SUMMcTavish], df_3260[:,:SUMBellevue], df_3260[:,:SUMAssomption], df_3260[:,:SUMStHubert], df_3260[:,:SUMPET], df_3260[:, :SUM2McTavish], df_3260[:, :SUM2Bellevue], df_3260[:, :SUM2Assomption], df_3260[:, :SUM2StHubert], df_3260[:, :SUM2PET], df_3260[:, :MAXMcTavish], df_3260[:, :MAXBellevue], df_3260[:, :MAXAssomption], df_3260[:, :MAXStHubert], df_3260[:, :MAXPET])\n",
    "X̃_3350 = hcat(ones(Float64,length(df_3350[:,1])),df_3350[:,:SUMMcTavish], df_3350[:,:SUMBellevue], df_3350[:,:SUMAssomption], df_3350[:,:SUMStHubert], df_3350[:,:SUMPET], df_3350[:, :SUM2McTavish], df_3350[:, :SUM2Bellevue], df_3350[:, :SUM2Assomption], df_3350[:, :SUM2StHubert], df_3350[:, :SUM2PET], df_3350[:, :MAXMcTavish], df_3350[:, :MAXBellevue], df_3350[:, :MAXAssomption], df_3350[:, :MAXStHubert], df_3350[:, :MAXPET])\n",
    "X̃_4240 = hcat(ones(Float64,length(df_4240[:,1])),df_4240[:,:SUMMcTavish], df_4240[:,:SUMBellevue], df_4240[:,:SUMAssomption], df_4240[:,:SUMStHubert], df_4240[:,:SUMPET], df_4240[:, :SUM2McTavish], df_4240[:, :SUM2Bellevue], df_4240[:, :SUM2Assomption], df_4240[:, :SUM2StHubert], df_4240[:, :SUM2PET], df_4240[:, :MAXMcTavish], df_4240[:, :MAXBellevue], df_4240[:, :MAXAssomption], df_4240[:, :MAXStHubert], df_4240[:, :MAXPET])\n",
    "X̃_4350 = hcat(ones(Float64,length(df_4350[:,1])),df_4350[:,:SUMMcTavish], df_4350[:,:SUMBellevue], df_4350[:,:SUMAssomption], df_4350[:,:SUMStHubert], df_4350[:,:SUMPET], df_4350[:, :SUM2McTavish], df_4350[:, :SUM2Bellevue], df_4350[:, :SUM2Assomption], df_4350[:, :SUM2StHubert], df_4350[:, :SUM2PET], df_4350[:, :MAXMcTavish], df_4350[:, :MAXBellevue], df_4350[:, :MAXAssomption], df_4350[:, :MAXStHubert], df_4350[:, :MAXPET])\n",
    "X̃_4380 = hcat(ones(Float64,length(df_4380[:,1])),df_4380[:,:SUMMcTavish], df_4380[:,:SUMBellevue], df_4380[:,:SUMAssomption], df_4380[:,:SUMStHubert], df_4380[:,:SUMPET], df_4380[:, :SUM2McTavish], df_4380[:, :SUM2Bellevue], df_4380[:, :SUM2Assomption], df_4380[:, :SUM2StHubert], df_4380[:, :SUM2PET], df_4380[:, :MAXMcTavish], df_4380[:, :MAXBellevue], df_4380[:, :MAXAssomption], df_4380[:, :MAXStHubert], df_4380[:, :MAXPET])\n",
    "\n",
    "df_3260.SURVERSE = predict(M_3260, X̃_3260)\n",
    "df_3350.SURVERSE = predict(M_3350, X̃_3350)\n",
    "df_4240.SURVERSE = predict(M_4240, X̃_4240)\n",
    "df_4350.SURVERSE = predict(M_4350, X̃_4350)\n",
    "df_4380.SURVERSE = predict(M_4380, X̃_4380)\n",
    "\n",
    "Ỹ = DataFrame(DATE=df_test[:,:DATE], NO_OUVRAGE=df_test[:,:NO_OUVRAGE], SURVERSE=zeros(Int64,length(df_test[:,1])))\n",
    "for i=1:length(df_test[:,1])\n",
    "    if df_test[i, :NO_OUVRAGE] == \"3260-01D\"\n",
    "        ind = findfirst(df_3260[:, :DATE] .== df_test[i, :DATE])\n",
    "        df_test[i,:SURVERSE] = df_3260[ind, :SURVERSE]\n",
    "    elseif df_test[i, :NO_OUVRAGE] == \"3350-07D\"\n",
    "        ind = findfirst(df_3350[:, :DATE] .== df_test[i, :DATE])\n",
    "        df_test[i,:SURVERSE] = df_3350[ind, :SURVERSE]\n",
    "    elseif df_test[i, :NO_OUVRAGE] == \"4240-01D\"\n",
    "        ind = findfirst(df_4240[:, :DATE] .== df_test[i, :DATE])\n",
    "        df_test[i,:SURVERSE] = df_4240[ind, :SURVERSE]\n",
    "    elseif df_test[i, :NO_OUVRAGE] == \"4350-01D\"\n",
    "        ind = findfirst(df_4350[:, :DATE] .== df_test[i, :DATE])\n",
    "        df_test[i,:SURVERSE] = df_4350[ind, :SURVERSE]\n",
    "    elseif df_test[i, :NO_OUVRAGE] == \"4380-01D\"\n",
    "        ind = findfirst(df_4380[:, :DATE] .== df_test[i, :DATE])\n",
    "        df_test[i,:SURVERSE] = df_4380[ind, :SURVERSE]\n",
    "    end\n",
    "end\n",
    "for i=1:length(df_test[:,1])\n",
    "    if df_test[i, :DATE] in dates_with_zero_mm\n",
    "        Ỹ[i, :SURVERSE] = 0\n",
    "    elseif df_test[i,:SURVERSE] > .5\n",
    "        Ỹ[i, :SURVERSE] = 1\n",
    "    else\n",
    "        Ỹ[i, :SURVERSE] = 0\n",
    "    end\n",
    "end\n",
    "\n",
    "# Pour chacune des lignes du fichier test, comportant un ouvrage et une date, une prédiction est requise.\n",
    "# Dans ce cas-ci, utilisons une prédiction les plus naîve. \n",
    "# On prédit avec une chance sur deux qu'il y ait surverse, sans utiliser de variables explicatives\n",
    "#n = size(test,1)\n",
    "#surverse = rand(n) .> .5\n",
    "\n",
    "\n",
    "# Création du fichier sampleSubmission.csv pour soumettre sur Kaggle\n",
    "ID = test[:,:NO_OUVRAGE].*\"_\".*string.(test[:,:DATE])\n",
    "sampleSubmission = DataFrame(ID = ID, Surverse=Ỹ[:,:SURVERSE])\n",
    "CSV.write(\"sampleSubmission_with_zero.csv\",sampleSubmission)\n",
    "\n",
    "# Vous pouvez par la suite déposer le fichier sampleSubmission.csv sur Kaggle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Julia 1.2.0",
   "language": "julia",
   "name": "julia-1.2"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
